```{r setup, include=FALSE}
options(width = 9999)
```

# Word Embeddings und NER

Bisher haben wir Worthäufigkeiten und das gemeinsame Vorkommen verschiedener Wörter in einem Korpus aus frequentistischer und probabilistischer Perspektive betrachtet. Um Worthäufigkeiten zu deuten und zu vergleichen, haben wir dabei untersucht, wie wahrscheinlich es ist, dass zwei Wörter in einem Korpus gemeinsam vorkommen. In diesem Kapitel untersuchen wir das gemeinsame Vorkommen von Wörtern in einem Korpus aus einer anderen mathematischen Perspektive, und zwar aus der Perspektive der linearen Algebra. Ausgangspunkt dieser Betrachtungsweise ist wieder die Feature Co-Occurrence Matrix (FCM). Aber die Zeilen und Spalten der Matrix werden jetzt nicht nur als Vektoren im Sinne von R aufgefasst, sondern als Vektoren im Sinne der linearen Algebra. Da jede Zeile der Matrix einem Wort im Korpus entspricht, beschreiben die Einträge in dem Wort-Vektor, wie oft die anderen Wörter im Korpus mit diesem Wort gemeinsam vorkommen. Wörter werden nach diesem Modell also durch ihren Kontext repräsentiert. 

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/jurafsky-martin_vector_representation.png")
```

Quelle: Jurafsky/Martin 2024, S. 113. [Hier](https://www.youtube.com/watch?v=xtD47WHNhxk) in Videoform.  

In die Sprache der linearen Algebra übersetzt (und geometrisch interpretiert) beschreibt jeder Wort-Vektor einen Punkt in einem mehrdimensionalen Raum. Dieser Raum hat nicht nur zwei oder drei Dimensionen, sondern hundert oder tausend. Diese höheren Dimensionen kann man sich geometrisch also nicht mehr vorstellen, aber es hilft, sich das Konzept für einen zweidimensionalen Raum zu veranschaulichen:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/jurafsky-martin_vector_space.png")
```

Quelle: Jurafsky/Martin 2024, S. 113. [Hier](https://www.youtube.com/watch?v=xtD47WHNhxk) in Videoform. 

Abstrakt könnt ihr euch diesen Raum als einen "semantischen Raum" vorstellen, in dem alle Wörter in einem Korpus eine bestimmte Position haben, und deren Entfernung voneinander davon abhängt, wie viele Einträge in den Vektoren übereinstimmen. Diese Auffassung von Wörtern als Vektoren erlaubt es, die Ähnlichkeit von zwei Wörtern mithilfe von **Ähnlichkeitsmaßen** wie der **Kosinus-Ähnlichkeit (cosine similariy)** zu untersuchen ([hier](https://youtu.be/-_i9Cg81rXA) eine kurze Erläuterung im Videoformat). Dieses Maß geht davon aus, dass ähnliche Wörter in einem semantischen Raum nah beieinander liegen (Jurafsky 2022). "Ähnlichkeit" wird hier gemessen über die Richtung und den Winkel von zwei Vektoren zueinander: Wenn zwei Vektoren dieselbe Richtung haben und der Winkel zwsichen den Vektoren sehr klein ist, dann haben sie einen ähnlichen Kontext, und sind sich ähnlich.

## Word Embeddings

Die Kookkurrenzmatrizen, mit denen wir bisher gearbeitet haben, eignen sich jedoch noch nicht für eine solche Darstellung. Erinnert euch daran, dass die meisten Einträge in jeder Zeile Nullen sind, weil Wörter nur mit einer kleinen Anzahl anderer Wörter gemeinsam vorkommen, aber die Matrix alle Wörter aus dem Korpus enthält. Diese Art von Vektoren werden "lang und spärlich" (long and sparse) genannt. Das Rechnen mit solchen Vektoren ist sehr aufwändig, und deswegen werden die Wort-Vektoren meist in "kurze und dichte" (short and dense) Vektoren umgewandelt, bevor Ähnlichkeitsmaße bestimmt werden (mehr dazu [hier](https://www.youtube.com/watch?v=FHhKmf1iPvw)). Solche "kurzen und dichten" Vektoren, die den Kontext eines bestimmten Wortes repräsentieren, werden **Word Embeddings** genannt. Word Embeddings können entweder direkt für eine Analyse verwendet werden, oder sie werden als Zwischenschritt in verschiedenen Methoden des "Natural Language Processing" verwendet. Im Folgenden werden wir beispielhaft zunächst Word Embeddings selbst erstellen und danach eine Methode kennenlernen, welche Word Embeddings als Zwischenschritt verwendet: die sogenannte Named Entity Recognition. Streng genommen haben wir sogar bereits mit POS Tagging und Dependency Parsing zwei Methoden kennengelernt, für die im Rahmen von UDPipe ebenfalls Word Embeddings verwendet werden. 

In diesem Abschnitt betrachten wir Word Embeddings zunächst als Analysewerkzeug. In der Literatur wird Word Embeddings oft die Eigenschaft zugeschrieben, die Bedeutung von Wörtern darzustellen: 

>"Embeddings represent meaning by leveraging the distributional hypothesis of language, an idea that goes back to at least Wittgenstein (1953). The distributional hypothesis has been succinctly summarized by an oft-repreated quote in this literature: 'you shall know a word by the company it keeps' (Firth, 1957). It posits that we can learn something about the semantic meanings of words on the basis of the words that appear frequently in a small context window around the focal word." (Grimmer et al. 2022, S. 79)

Dabei kann die in diesem Sinne verstandene semantische Bedeutung eines Wortes auf **Type- oder auf Tokenebene** erfasst werden: Entweder, ein Wort wird als Type betrachtet. Dann ist der Kontext jedes Wort, das im gesamten Korpus in einem bestimmten Abstand ("context window") mit diesem Wort gemeinsam vorkommt. Oder ein Wort wird als Token betrachtet und der Kontext ist für jedes Vorkommen verschieden. Die erste Art der Word Embeddings auf Type-Ebene wird als **"Static Word Embeddings"** bezeichnet. Embeddings auf Token-Ebene werden als "Contextualized" oder **"Contextual Word Embeddings"** bezeichnet. Daneben gibt es noch **Character Embeddings**, die (wie auch im Rahmen von UDPipe) häufig mit Word Embeddings im engeren Sinne kombiniert werden. Character Embeddings repräsentieren den Kontext einzelner Zeichen, also zum Beispiel alle Buchstaben, mit denen der Buchstabe "a" in einem bestimmten Kontextfenster vorkommt. 

Mit unserer kleinen Beispielanalyse zu den Grimm'schen Märchen sind wir ja schon fertig, aber wir betrachten im Folgenden noch ein letztes Mal die mittlerweile vermutlich leidigen, aber dafür bereits vertrauten Märchen.

```{r eval=FALSE}
library(readtext)
library(quanteda)

# Märchen von 1857 und 1812/15 einlesen und Informationen aus dem Dateinamen extrahieren
maerchen_alle <- readtext("maerchen_alle/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Titel", "Jahr"), encoding = "UTF-8")

```

```{r echo=FALSE, results=FALSE, message=FALSE, warning=FALSE}
library(readtext)
library(quanteda)
maerchen_alle <- readtext("data/maerchen_alle/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Titel", "Jahr"), encoding = "UTF-8")
```

```{r}
maerchen_corpus <- corpus(maerchen_alle) %>%
  corpus_subset(Jahr == 1857)
```


<details>
<summary><b>Anwendungsbeispiele: Word Embeddings</b></summary>

- Nikhil Garg, Londa Schiebinger, Dan Jurafsky und James Zou (2018), Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, https://www.pnas.org/doi/10.1073/pnas.1720347115.
- Sandeep Soni, Lauren F. Klein und Jacob Eisenstein (2021). Abolitionist Networks. Modeling Language Change in Nineteenth-Century Activist Newspapers, https://doi.org/10.22148/001c.18841.
- Francisco A Borja (2023), Mining for meaning. How text mining can uncover the French Liberal School’s key ideas, https://doi.org/10.1093/llc/fqad084.

</details>

### Count-Based Word Embeddings mit Quanteda und PMI-Werten

Zunächst schauen wir uns an, wie aus den bereits bekannten Quanteda Feature-Cooccurrence-Matrizen (FCM) Word Embeddings erstellt werden können. Dazu nutzen wir das bereits aus dem Kapitel "Textanalyse III" bekannte Assoziationsmaß Pointwise Mutual Information (PMI), um die Stärke der Assoziation zwischen Wörtern in der Feature Co-Occurrence Matrix zu gewichten. Auf dieser Basis reduzieren wir in einem zweiten Schritt die Dimensionen der Matrix, sodass wir dichte Wort-Vektoren, also Word Embeddings, erhalten. Zuletzt messen wir die semantische Ähnlichkeit von einigen Beispielwörtern aus dem Märchenkorpus anhand der Kosinusähnlichkeit ihrer Embedding-Vektoren. Für eine Erläuterung zur Kosinusähnlichkeit und zur Methode SVD (Singular Value Decomposition), die zur Umwandlung von spärlichen in dichte Matrizen verwendet wird, verweise ich auf die optionalen Abschnitte zum mathematischen Hintergrund am Ende dieses Abschnitts. 

Code adaptiert von Stoltz/Taylor 2024, Mapping Texts, S. 210-214.

```{r}
# Funktion cos_sim() aus Stoltz/Taylor 2024, S. 91. 
cos_sim <- function(A, B) {
  sum(A * B) / sqrt(sum(A^2) * sum(B^2))
}

# Funktion fcm_pmi() von Kohei Watanabe (s. Abschnitt "Pointwise Mutual Information")
fcm_pmi <- function(x) {
  m <- x@meta$object$margin
  x <- as(x, "TsparseMatrix") #"dgTMatrix"
  x@x <- log(x@x / (m[x@i + 1] * m[x@j + 1]) * sum(m))
  x@x[x@x < 0] <- 0
  as.fcm(x)
}
```

```{r warning=FALSE, message=FALSE}
library(quanteda)

# Sätze als Dokumente festlegen
maerchen_sentences <- corpus_reshape(maerchen_corpus, to="sentences")
maerchen_toks <- tokens(maerchen_sentences, remove_punct=TRUE) %>%
  tokens_remove(pattern = stopwords("de")) %>%
  tokens_remove(pattern = "dass") 

# seltene Kookkurrenzen herausfiltern
feats <- dfm(maerchen_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()
maerchen_toks <- tokens_select(maerchen_toks, feats, padding = FALSE)

# FCM erstellen und nach PPMI-Werten gewichten
maerchen_fcm <- fcm(maerchen_toks, context="document")
maerchen_pmi <- fcm_pmi(maerchen_fcm)
```
```{r}
# spärliche Matrix in dichte Matrix umwandeln
maerchen_svd <- svd(maerchen_pmi)
# saveRDS(maerchen_svd, "maerchen_svd.rds")
```

```{r}
# Dimensionen auswählen 
maerchen_svd <- maerchen_svd$v[, 1:100]  
# Zeilennamen wieder hinzufügen
rownames(maerchen_svd) <- rownames(maerchen_pmi)
```


```{r}

vec <- maerchen_svd["Königstochter", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Königssohn", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Frosch", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Fuchs", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Schloss", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Tochter", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["ging", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)


```

Für den mathematischen Hintergrund zu PMI verweise ich auf den Abschnitt [8.4.4.2 "Pointwise Mutual Information (PMI)"](https://lipogg.github.io/textanalyse-mit-r/textanalyse-iii-wortfrequenzanalysen.html#pointwise-mutual-information-pmi).

<details>
<summary><b>Mathematischer Hintergrund: SVD </b></summary>

SVD steht für Singular Value Decomposition. Es handelt sich dabei um eine Methode der sogenannten Matrixfaktorisierung. Eine Matrixfaktorisierung ist die Zerlegung einer Matrix in Teilmatrizen. 

Hintergrund folgt noch. 


</details>
<details>
<summary><b>Mathematischer Hintergrund: Kosinusähnlichkeit </b></summary>

Hervorragende Erläuterungen zur Kosinusähnlichkeit bieten: 

* Jan Jurafsky (2022). Video-Tutorial Cosine Similarity, Stanford University, https://www.youtube.com/watch?v=-_i9Cg81rXA.
* Christopher Potts (2021). Vorlesungsmitschnitt "Natural Language Understanding", Abschnitt "Vector Comparison", Stanford University, https://www.youtube.com/watch?v=eKvbYOc2rOs.
* Justin Grimmer, Margaret E. Roberts und Brandon M. Stewart (2022). Text as Data. A New Framework for Machine Learning and the Social Sciences, ch. 7: The Vector Space Model and Similarity Metrics, pp. 70-77.
* Dustin S. Stoltz und Marshall A. Taylor (2024). Mapping Texts. Computational Text Analysis for the Social Sciences, ch. 4: Math Basics. Comparing Vectors, pp. 34-39. 

</details>


### Prediction-Based Static Word Embeddings mit Quanteda und GloVe 

Ein weiteres bekanntes Verfahren zur Erstellung von Word Embeddings ist GloVe (Global Vectors for Word Representation). Ähnlich wie die Count-basierten Word Embeddings im vorherigen Abschnitt baut GloVe auf der Idee auf, dass die statistische Information über das gemeinsame Vorkommen von Wörtern im gesamten Korpus genutzt werden kann, um die semantische Ähnlichkeit von Wörtern mathematisch zu modellieren. Ausgangspunkt ist auch wieder eine Kookkurrenzmatrix, in der gezählt wird, wie oft zwei Wörter innerhalb eines bestimmten Kontextfensters gemeinsam auftreten. Aber anders als bei dem im vorhergehenden Abschnitt dargestellten count-basierten Verfahren wird diese Matrix nicht direkt analysiert, sondern sie dient als Grundlage für ein Machine-Learning-Modell, das Word Embeddings für alle Wörter im Korpus "lernt". Für eine detaillierte Erläuterung verweise ich auf den optionalen Abschnitt "mathematischer Hintergrund" am Ende des Abschnitts. 

Code nach https://quanteda.io/articles/pkgdown/replication/text2vec.html

```{r}
library(quanteda)
library(quanteda.textstats)
library(text2vec)

maerchen_toks <- tokens(maerchen_corpus)

feats <- dfm(maerchen_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()

maerchen_toks <- tokens_select(maerchen_toks, feats, padding = TRUE)
maerchen_fcm <- fcm(maerchen_toks, context = "window", count = "weighted", weights = 1 / (1:5), tri = TRUE)

glove <- GlobalVectors$new(rank = 50, x_max = 10)
wv_main <- glove$fit_transform(maerchen_fcm, n_iter = 10,
                               convergence_tol = 0.01, n_threads = 8)

wv_context <- glove$components
dim(wv_context)

word_vectors <- wv_main + t(wv_context) # sum up u and v vectors (column and row vectors), https://youtu.be/ASn7ExxLZws?si=W8MGBXKGDF3E0WVK&t=2519

vec <- word_vectors["Königstochter", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Königssohn", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Schloss", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Wald", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

```

Ein Beispiel zu Contextual Word Embeddings folgt noch.

<details>
<summary><b>Mathematischer Hintergrund: GloVe </b></summary>

Der Name GloVe verweist auf die Grundidee des Modells: In dem Forschungsartikel, in dem das Modell zum ersten Mal vorgestellt wurde, schreiben die Autor:innen, dass GloVe für "Global Vectors" steht,  "because the global corpus statistics are captured directly by the model" ([Pennington, Socher und Manning 2014](https://doi.org/10.3115/v1/D14-1162), S.2). Mit "global corpus statistics" ist im Grunde eine Kookkurrenzmatrix aller Wörter (oder "Features") im Korpus gemeint, in der die Anzahl der gemeinsamen Vorkommen von Wortpaaren innerhalb eines Kontextfensters von $m$ Wörtern erfasst ist. Kookkurrenzmatrizen kennt ihr bereits, im Quanteda-Kontext sind das die Feature Co-Occurrence Matrizen (FCM). GloVe nutzt aber nicht direkt diese Kookkurrenzmatrizen, um Word Embeddings zu erzeugen, wie es im vorhergehenden Abschnitt "Count-based Word Embeddings" der Fall war, sondern die Kookkurrenzstatistiken werden nur als Input für ein spezielles Regressionsmodell verwendet, also ein Modell, das auf der Grundlage von bekannten Werten für eine bestimmte Variable (Alter, Ausbildung,...) die wahrscheinlichsten Werte für eine andere, von dieser Variable abhängigen Variable (z.B. Einkommen) vorhersagt. Die Autor:innen bezeichnen GloVe selbst als ein "global log-bilinear regression model". Das erscheint auf den ersten Blick weit entfernt von Word Embeddings, tatsächlich sind die Word Embeddings aber die Parameter des Modells. Ziel des GloVe-Modells ist es, diese Parameter so bestimmen, dass die vorhergesagten Kookkurrenzen möglichst gut mit den tatsächlichen  Kookkurrenzhäufigkeiten in der Kookkurrenzmatrix übereinstimmen. Die Parameter werden durch einen Trainings- bzw. Optimierungsprozess geschätzt, der darin besteht, dass eine gewichtete Least-Squares-Kostenfunktion ("weighted least squares") mithilfe des sogenannten Gradientenverfahrens ("gradient descent") minimiert wird. Was bedeutet das? Nähern wir uns den Begriffen schrittweise an. 

Statt wie bei einer "klassischen" Regression zum Beispiel das wahrscheinlichste Einkommen aus Alter und Ausbildung vorherzusagen, wird durch das hier verwendete Regressionsmodell vorhergesagt, wie oft ein Wort $k$ im Kontext von einem Wort $i$ vorkommt, also die Anzahl der Kookkurrenzen der beiden Wörter ("cooccurrence count", $X_{ik}$) in einer Kookkurrenzmatrix $X$. Genauer gesagt wird eigentlich der Logarithmus von dieser Anzahl vorhergesagt, also $\log(X_{ik})$. Und die Variablen, aus denen der Wert $\log(X_{ik})$ hervorgesagt wird, sind nicht Alter und Ausbildung, sondern die Embedding-Vektoren der beiden Wörter $i$ und $k$ ("word vectors") selbst, $w_i$ und $\tilde{w}_k$. Aus diesen beiden Vektoren wird das Skalarprodukt gebildet, $w_i^\top \tilde{w}_k$, das hier einfach durch die Multiplikation des transponierten Vektors $w_i$ mit dem Vektor $\tilde{w}_k$ berechnet wird ([dieses Video](https://www.youtube.com/watch?v=sQ1b3KaO_6Y) erklärt diese Operation im Detail). Das "Transponieren" haben wir bereits bei der Umformung von Matrizen kennengelernt und dazu die R-Funktion `t()` verwendet. Ganz genau gesagt wird also der Logarithmus der Anzahl der Kookkurrenzen aus dem Skalarprodukt der Embedding-Vektoren hervorgesagt. Das Skalarprodukt wird aber noch mit zwei Konstanten, den sogenannten Bias-Termen ("bias terms"), verrechnet. Das sind einfach Zahlen, die die Vorhersage verbessern sollen, und die uns erst einmal nicht weiter interessieren. Das Regressionsmodell sieht nach den bisherigen Ausführungen so aus: 

$w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik})$

Dieses Modell behandelt aber seltene und sehr häufige Kookkurrenzen genau gleich, obwohl sehr seltene Kookkurrenzen eigentlich nicht sehr informativ sind. Gleichzeitig sollte aber die Bedeutung von sehr häufigen Kookkurrenzen auch nicht überbewertet werden. Deswegen ergänzen die Autor:innen das Modell um eine spezielle Funktion ("weighting function", $f(X_{ij})$)), die bewirkt, dass sehr häufige und sehr seltene Kookkurrenzen "umgewichtet" werden, damit sie weder zu wenig noch zu viel Gewicht in der Berechnung haben. 

Nun ist $log(X_{ik})$ aber ja bereits aus der Kookkurrenzmatrix für das Korpus bekannt und muss gar nicht wie in einer klassischen Regression vorhergesagt werden. Stattdessen ist das eigentliche Ziel, Embedding-Vektoren $w_i$ und $\tilde{w}_k$ zu finden, deren Skalarprodukt die reale Anzahl der Kookkurrenzen im Korpus möglichst genau vorhersagt. Wenn das Modell diese Vektoren "gut" geschätzt hat, dann wird die reale Kookkurrenzhäufigkeit ziemlich genau vorhergesagt, und dann ist die Differenz zwischen der Vorhersage $w_i^T \tilde{w}_k + b_i + \tilde{b}_k$ und den realen Kookkurrenzhäufigkeiten $log(X_{ik})$ für zwei Wörter $i$ und $k$ sehr klein. Um Vektoren zu finden, für die dieser Abstand so klein wie möglich ist, wird das Regressionsmodell als "least squares"-Problem umformuliert. Das Regressionsmodell, das wir vorhin aufgestellt haben, dient also eigentlich nur als Referenz, um einzuschätzen, wie "gut" die Vektoren sind. Die least squares-Methode beschreibt ein Vorgehen, bei dem die Summe der quadrierten Differenz zwischen den in dem  Regressionsmodell vorhergesagten Werten und den realen, in einem konkreten Datensatz beobachteten Werten minimiert wird. Für eine anschauliche Erklärung, warum das in diesem Kontext so gemacht wird, empfehle ich [dieses Video](https://www.youtube.com/watch?v=EuBBz3bI-aA). Mathematisch wird dieses durch die Gewichtungsfunktion $f(X_{ij})$ gewichtete "least squares" Modell also schlussendlich so formuliert: 

$J = \sum_{i,j=1}^V f(X_{ij}) \left(w_i^\top \tilde{w}_j + b_i + \tilde{b}_j- \log X_{ij} \right)^2$

Hierbei ist $X_{ij}$ die Kookkurrenzhäufigkeit von zwei beliebigen Wörtern $i$ und $j$, $w_i$ und $\tilde{w}_j$ sind die zu lernenden Vektoren, $b_i$ und $\tilde{b}j$ die Bias-Terme und $f(X{ij})$ die Gewichtungsfunktion, die, wie bereits erläutert, seltene und häufige Kookkurrenzen unterschiedlich stark berücksichtigt. Das Ergebnis dieser Berechnung wird quadriert und aufsummiert. Die Formel gilt jetzt also nicht mehr für zwei konkrete Wörter $i$ und $k$, sondern gesucht wird der Minimalwert über alle Wörter im Korpus hinweg. So eine Funktion, die die Differenz zwischen Vorhersage und tatsächlichen Werten (oder "Zielwerten") über alle Werte (hier Wortpaare) hinweg beschreibt, wird auch **Kostenfunktion ("cost function")** genannt. Der Ausdruck innerhalb der Summe, der die Differenz zwischen Vorhersage und tatsächlichen Werten für einzelne Werte (bzw. Wortpaare) beschreibt, wird auch **Verlustfunktion ("loss function")** genannt. Die konkrete Methode, die zur Minimierung der Funktion verwendet wird, ist das sogenannte Gradientenverfahren ("Gradient Descent"), aber das werden wir hier nicht vertiefen. Für uns reicht zu verstehen, dass mithilfe des Gradientenverfahrens die Kostenfunktion schrittweise minimiert wird. Dieser Minimierungsprozess wird beim Ausführen der Funktion `glove$fit_transform()` im Beispiel oben durch den Konsolen-Output registriert (`"## INFO  [17:48:55.374] epoch 1, loss 0.1953"`). Genau dieser Minimierungsprozess ist das, was beim maschinellen Lernen mit "Training" gemeint ist, und weil es dabei darum geht, durch eine gute Wahl der Embedding-Vektoren die bestmöglichen Vorhersage zu treffen, ist das Ziel dieses Prozesses die "Optimierung" des Modells. 

Die GloVe-Kostenfunktion wird manchmal auch in leicht vereinfachter und allgemeiner Form angegeben:  

$J(\theta) = \frac{1}{2} \sum_{i,j=1}^W f(P_{ij}) \left( u_i^T v_j - \log P_{ij} \right)^2$

Das Grundprinzip ist dasselbe, nur, dass nicht mit der Anzahl der Kookkurrenzen von beliebigen Wörtern $i$ und $j$ direkt gerechnet wird, sondern mit der Wahrscheinlichkeit $P_{ij}$, dass $j$ im Kontext von $i$ vorkommt. Die Wahrscheinlichkeit berechnet sich dabei aus der Anzahl der Kookkurrenzen. Die Wahrscheinlichkeit, dass ein konkretes Wort $k$ im Kontext von $i$ vorkommt, berechnet sich beispielsweise mit $P_{ik}=P(k|i)={\frac {X_{ik}}{X_{i}}} = {\frac {X_{ik}}{\sum_{k}X_{ik}}}$, wobei $X_i= \sum_{k}X_{ik}$ die Gesamtzahl der Kontextwörter, die mit $i$ gemeinsam vorkommen, beschreibt. Vielleicht ist euch aufgefallen, dass die Größe des Kontextfensters $m$, für das die Kookkurrenzen bestimmt werden, sich in keiner der Formeln wiederfindet. Dabei handelt es sich deswegen um einen sogenannten "Hyperparameter", der außerhalb der mathematischen Formulierung festgelegt wird, aber dennoch Einfluss auf die Berechnung hat. 

Eine Anmerkung zum Schluss: Zwar haben habe ich der Verständlichkeit halber hier immer von "vorhersagen" gesprochen, aber im Kontext des maschinellen Lernens spricht man eigentlich von "**approximieren**". Für uns sind diese Begrifflichkeiten nicht ganz so wichtig, aber wenn ihr in der Fachliteratur nachschlagt, werdet ihr häufig auf diesen Begriff stoßen. Es ist außerdem wichtig anzumerken, dass die Entwickler:innen des GloVe-Modells zwar von "Regression" sprechen, dass sie damit aber nicht dasselbe meinen wie eine statistische Regression, sondern nur den allgemeinen Ansatz, Zusammenhänge verschiedener Variablen zu modellieren.

Zusammengefasst: Ziel des GloVe-Modells ist es, für jedes Wort einen Embedding-Vektor zu "lernen", sodass das Skalarprodukt der Vektoren zweier Wörter möglichst genau den Logarithmus der tatsächlichen Kookkurrenzhäufigkeit dieser beiden Wörter vorhersagt. Weil für die Vorhersage der Kookkurrenzen nur die "Rohdaten" aus dem Korpus selbst, und keine Zielwerte aus einem externen, bereits annotierten Korpus verwendet werden (wie es beispielsweise beim POS Tagging und Dependency Parsing der Fall war), gilt GloVe als Methode des sogenanneten **unsupervised machine learning**.

Um die Mathematik hinter GloVe tiefergehend zu verstehen, braucht ihr allerdings ab diesem Punkt grundlegende Kenntnisse in Statistik und Wahrscheinlichkeitstheorie (Relative Häufigkeiten, Bedingte Wahrscheinlichkeit), linearer Algebra (Matrizen, Vektoren, Matrixoperationen, Skalarprodukt) und Analysis (Logarithmus, Ableitungen). Wenn ihr bereits grundlegende Vorkenntnisse in diesen Bereichen mitbringt, empfehle ich die folgenden Mitschnitte aus der Vorlesung "Natural Language Processing with Deep Learning" von Christopher Manning und Richard Socher zur Vertiefung: 

* Lecture 2: Word Vectors, https://www.youtube.com/watch?v=ERibwqs9p38
* Lecture 3: GloVe: Global Vectors for Word Representation, https://www.youtube.com/watch?v=ASn7ExxLZws

Die Vorlesungen gehören zur [Lehrveranstaltung "Natural Language Processing with Deep Learning"](https://web.stanford.edu/class/cs224n/) an der Universität Stanford. 

Begleitend empfehle ich die Lektüre des Forschungsartikels, in dem GloVe erstmals vorgestellt wurde: 

* Jeffrey Pennington, Richard Socher, and Christopher Manning (2014),  GloVe: Global Vectors for Word Representation, https://doi.org/10.3115/v1/D14-1162.

</details>

## Named Entity Recognition (NER)

Word Embeddings sind im Bereich Natural Language Processing oft nur ein Schritt in einer Reihe von Textverarbeitungsmethoden. Mithilfe von statischen und kontextuellen Word Embeddings (aber eher nicht mit unseren simplen count-based embeddings) können sogenannte "benannte Entitäten" in Texten identifiziert werden, also Wörter, die auf eine bestimmte Art von Entität verweisen: Personen, Orte, Organisationen sind die gängigsten Beispiele:

> "A **named entity** is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of **named entity recognition (NER)** is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: **PER** (person), **LOC** (location), **ORG** (organization), or **GPE** (geo-political entity). However, the term **named entity** is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices."
([Jurafsky/Martin 2023, Ch.8, S.6](https://web.stanford.edu/~jurafsky/slp3/8.pdf))

Ähnlich wie beim POS Tagging geht es also bei der Named Entity Recognition (NER) darum, bestimmte Tokens (oder auch Tokenkombinationen) automatisiert zu identifizieren und mit einem Tag oder Label zu versehen. Anders als beim POS Tagging ist das Ziel bei der NER aber, semantische Strukturen zu erkennen und nicht syntaktische. Named Entity Recognition Systeme machen sich zur Erkennung der Entitäten die Eigenschaft von Word Embeddings zunutze, dass Wörter, die in ähnlichen semantischen Kontexten vorkommen (z.B. Städte, Personen, ...), im mehrdimensionalen Raum enger beeinanderliegen als Wörter, deren Kontexte sehr verschiedenen sind. 

Im folgenden werden beispielhaft zwei der populärsten NER-Systeme vorgestellt: SpaCy und Flair. Beide sind eigentlich für Python entwickelt, können aber mithilfe von sogenannten "Wrappern" auch aus R heraus angewandt werden. Beide Systeme nutzen Word Embeddings in Kombination mit einer Reihe anderer Verarbeitungsschritte, um Entitäten zu erkennen: SpaCy nutzt statische Embeddings (zumindest für deutsche Texte) und FlaiR nutzt kontextuelle Embeddings (allerdings nicht mehr ELMo, sondern eigentlich Character Embeddings, also Vektoren, die den Kontext einzelner Buchstaben repräsentieren). Dabei ist wichtig zu beachten, dass die Embeddings bei der hier vorgestellten simplen Anwendung dieser Systeme nicht alle auf der Grundlage der Eingabetexte erstellt werden, sondern dass ähnlich wie beim POS-Tagging andere Textkorpora verwendet werden, um Word Embeddings zu generieren und ein Sprachmodell zu trainieren, das Named Entities erkennen kann. 

Als Beispiel dient uns diesmal ein sehr kleines Korpus von Briefen aus der Korrespondenz von Karl Marx und Friedrich Engels aus der [digitalen Edition der Marx-Engels-Gesamtausgabe](https://megadigital.bbaw.de).  


```{r}
library(readtext)
marx_test <- readtext(file="./data/marx_briefe/Marx_Engels_London_25-1-1868.txt")

marx_briefe <- readtext(file="./data/marx_briefe/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Von", "An", "Ort", "Datum"), encoding = "UTF-8")

```

<details>
<summary><b>Anwendungsbeispiele: NER</b></summary>

- Lisa Mendelman and Anna Mukamal (2021). The Generative Dissensus of Reading the Feminist Novel, 1995-2020:  A Computational Analysis of Interpretive Communities, https://doi.org/10.22148/001c.30009.
- Cécile Armand (2024). Shaping the Transnational Public Sphere in Republican China. Discourses and Practices of the Rotary Club in the Shanghai Press (1919-1949), https://journalofdigitalhistory.org/en/article/69Xry3ztPAk5 ([hier](https://journalofdigitalhistory.org/en/article/69Xry3ztPAk5?idx=94&layer=hermeneutics&lh=860&pidx=94&pl=narrative&y=85) ist der Abschnitt, in dem es um NER geht). 

</details>

## Beispiel mit SpaCy / Spacyr

Python Setup 

```{r eval=FALSE}
install.packages("spacyr")
library("spacyr")

spacy_install(version = "apple")
```

```{r echo = FALSE, warning=FALSE, message=FALSE}
library("spacyr")
reticulate::use_python("./nlp-env/bin/python")
reticulate::use_condaenv("nlp-env")

```

Sprachmodell herunterladen und initialisieren

```{r eval=FALSE}
spacy_download_langmodel("de_core_news_lg")
```
```{r warning=FALSE}
spacy_initialize(model = "de_core_news_lg")
```

Jetzt können wir beginnen: 

```{r}
results <- spacy_parse(marx_test, lemma = FALSE, entity = TRUE)
results_entities <- entity_extract(results)
results_entities # View(results_entities)
```


```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("images/ner_spacy_results.png")
```

Quelle: https://megadigital.bbaw.de/briefe/detail.xql?id=M0000533.


```{r}
# Spalte doc_id des readtext-Dataframes ist per Default Dateiname; das ersetzen wir durch das Datum 
marx_briefe$doc_id <- marx_briefe$Datum
```

```{r attr.output='style="max-height: 200px;"'}
results_briefe <- spacy_parse(marx_briefe, lemma = FALSE, entity = TRUE)

briefe_entities <- entity_extract(results_briefe)
briefe_entities

briefe_pers <- briefe_entities[briefe_entities$entity_type == "PER",]
briefe_pers
```

```{r warning=FALSE, message=FALSE}
library(ggplot2)

marx_pers_plot <- ggplot(briefe_pers, aes(x = doc_id, y = entity)) +
  geom_point(alpha=0.6) +
  theme(axis.text.y = element_text(size=3), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5)) + 
  labs(x = "Brief", y = "Person")

library(plotly)
ggplotly(marx_pers_plot, height=800)

```

```{r eval=FALSE}
spacy_finalize()

```

* Spacyr Dokumentationsseite: 
* Spacyr Tutorial: https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html
* Spacy Dokumentationsseiten: https://spacy.io/usage
* Spacy Sprachmodelle: https://spacy.io/usage/models und https://spacy.io/models/de#de_core_news_lg

## Beispiel mit Flair / FlaiR


```{r eval=FALSE}
install.packages("reticulate")
```

```{r eval=FALSE}
reticulate::use_python("/Path/to/python") # >= 3.10!
reticulate::py_config() # Einstellungen überprüfen 
```

```{r eval=FALSE}
install.packages("remotes")
remotes::install_github("davidycliao/flaiR") # force=TRUE
```


```{r warning=FALSE, message=FALSE}
library(flaiR)
```

```{r}
tagger_ner <- load_tagger_ner("de-ner-large")
results <- get_entities(text=marx_test$text, doc_ids=marx_test$doc_id, tagger_ner)
results

```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("images/ner_flair_results.png")
```

Quelle: https://megadigital.bbaw.de/briefe/detail.xql?id=M0000533.


```{r attr.output='style="max-height: 200px;"'}
results_briefe <- get_entities(text=marx_briefe$text, doc_ids=marx_briefe$Datum, tagger_ner)
results_briefe # View(results_briefe)

results_pers <- results_briefe[results_briefe$tag == "PER",]
results_pers
```

```{r warning=FALSE, message=FALSE}
library(ggplot2)

marx_pers_plot <- ggplot(results_pers, aes(x = doc_id, y = entity)) +
  geom_point(alpha=0.6) +
  theme(axis.text.y = element_text(size=3), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5)) + 
  labs(x = "Brief", y = "Person")

library(plotly)
ggplotly(marx_pers_plot, height=800)
```



```{r echo=FALSE}

spacy_finalize()

```

* FlaiR Dokumentationsseite: https://github.com/davidycliao/flaiR
* Flair Dokumentationsseiten: https://flairnlp.github.io/docs/intro und  https://github.com/flairNLP/flair
* Flair-Sprachmodelle: https://flairnlp.github.io/docs/tutorial-basics/tagging-entities#list-of-ner-models und  https://huggingface.co/flair/ner-german-large
* ConLL 2003 Datensatz: https://huggingface.co/datasets/conll2003 und https://www.clips.uantwerpen.be/conll2003/ner/

Die mithilfe von FlaiR oder Spacyr erkannten Entitäten können natürlich auch noch auf etwas elegantere Weise visualisiert werden. Eine Möglichkeit ist eine [Treemap](https://www.data-to-viz.com/graph/treemap.html), die in unserem Beispiel auf einen Blick zeigt, wie oft welche Ortsangabe in welchem Brief erwähnt wird ("nested treemap" links) und wie oft jede Ortsangabe insgesamt in allen Briefen vorkommt ("flat treemap" rechts). Um die Treemap zu erstellen, Dazu wollen wir den Code aus dem Abschnitt "Nested Layers in Treemap" in den  [Plotly-Dokumentationsseiten](https://plotly.com/r/treemaps/) anpassen. Dazu müssen wir unsere Input-Daten erst einmal in dieselbe Form bringen wie die beiden CSV-Tabellen, die zur Erstellung der Visualisierung in den Dokumentationsseiten verwendet werden:  

```{r message=FALSE, warning=FALSE, attr.output='style="max-height: 200px;"'}

# Tabelle mit Named Entities nach Ortsangaben filtern
results_loc <- results_briefe[results_briefe$tag == "LOC",]
results_loc

# Inputdaten für "flat" treemap (rechts) vorbereiten 
entity_counts <- as.data.frame(table(results_loc$entity))
names(entity_counts) <- c("ids", "value")
flattm_df <- entity_counts[entity_counts$value > 0, ]
flattm_df$parents <- ""
flattm_df$labels <- flattm_df$ids
flattm_df

# Inputdaten für "nested" treemap (links) vorbereiten 
# Erste Ebene: Kacheln für die Briefe
letters_unique <- unique(results_loc$doc_id)
level_1 <- data.frame(ids=letters_unique,
                      labels=letters_unique,
                      parents="",
                      value=0)

# Zweite Ebene: Kacheln für Ortsangaben je Brief
entity_per_letter <- as.data.frame(table(results_loc$doc_id, results_loc$entity))
names(entity_per_letter) <- c("parents", "labels", "value")
level_2 <- entity_per_letter[entity_per_letter$value > 0, ]
level_2$ids <- paste0(level_2$parents, "/", level_2$labels)

nestedtm_df <- rbind(level_1, level_2)
nestedtm_df
```

Jetzt kann die eigentliche Visualisierung erstellt werden. Wir ergänzen dabei den Code aus den Dokumentationsseiten um die Zeilen `marker = list(colorscale="Reds")` und `marker = list(colorscale="Greys", reversescale =T)`, da die Kacheln per Default bunt sind, aber die bunte Farbe inhaltlich für unsere Daten keine Bedeutung hat. Der verschachtelten Treemap auf der linken Seite fügen wir außerdem mit der Zeile `values = nestedtm_df$value,` die Anzahl der erwähnten Ortsangaben je Brief hinzu, da diese Information für den Abgleich mit der Gesamtanzhal der erwähnten Ortsangaben wichtig ist:  

```{r message=FALSE, warning=FALSE}
library(plotly)

# Plotly Treemap-Objekt für die "nested" treemap (links) erstellen
fig <- plot_ly(
  type = "treemap",
  ids = nestedtm_df$ids,
  labels = nestedtm_df$labels,
  parents = nestedtm_df$parents,
  values = nestedtm_df$value,
  domain = list(column = 0),
  marker = list(colorscale="Reds")
)

# Treemap-Objekt für die "flat" treemap (rechts) hinzufügen
fig <- fig %>% add_trace(
  type = "treemap",
  ids = flattm_df$ids,
  labels = flattm_df$labels,
  parents = flattm_df$parents,
  values = flattm_df$value,
  maxdepth = 1,
  domain = list(column = 1),
  marker = list(colorscale="Greys", reversescale =T)
)

# Layout einstellen
fig <- fig %>% layout(grid = list(columns = 2, rows = 1))

# Fertige Visualisierung anzeigen lassen
fig


```

:::task
Verständnisfragen:

- Wie könnte man die Ergebnisse der Named Entity Recognition noch visualisieren? Nutzt als Inspiration die Seite https://www.data-to-viz.com/
- Überlegt euch eine Forschungsfrage, die mithilfe von Named Entities beantwortet werden könnte. 
- Welches der beiden NER-Systeme würdet ihr verwenden, wenn ihr möglichst viele Kandidaten für Named Entities identifizieren wollt, die ihr später manuell überprüft?   

:::


## Vergleich und Ausblick

Neben Spacyr und FlaiR gibt es noch weitere Pakete, die es erlauben, Named Entity Recognition in R durchzuführen. Dazu zählt zum Beispiel das Paket nametagger, das von demselben Personenkreis entwickelt wurde, die auch UDPipe entwickelt haben, und verschiedene Pakete, mit denen das an der Universität Stanford entwickelte NER-System CoreNLP verwendet werden kann (laut [Dokumentationsseite](https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran) wird CoreNLP von den R Paketen [CleanNLP](https://github.com/statsmaths/cleanNLP), [NLP](https://cran.r-project.org/web/packages/NLP/) und [CoreNLP](https://cran.r-project.org/web/packages/coreNLP/) unterstützt). Das Paket CleanNLP  erlaubt, verschiedene Natural Language Processing Pipelines aus R heraus zu verwenden und diese in einem "Tidy" Datenformat abzubilden (mehr dazu [hier](https://arxiv.org/pdf/1703.09570.pdf)).

Nametagger unterstützt aktuell nur NER für englischsprachige Dokumente (siehe [diesen Beitrag](https://bookdown.org/morleyjamesweston/MCM4TM/named-entity-recognition.html#basic-machine-learning-approaches) für ein Anwendungsbeispiel), und während CoreNLP für moderne deutschsprachige Texte [sehr gute Ergebnisse erzielt](https://github.com/MaviccPRP/ger_ner_evals), wurde gezeigt, dass Flair für historsiche Dokumente bessere Ergebnisse erreichen kann (zum Beispiel [hier](https://hal.science/hal-04056513/document) und [hier](https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/44844/Holmes_washington_0250O_20778.pdf)).

Aber für die Out-of-the-Box NER Modelle gilt genauso wie für die Modelle für das POS Tagging und Dependency Parsing: Auch, wenn manchmal Modelle, die auf modernen (Zeitungs-)texten trainiert sind, bei der NER historischer Dokumente brauchbare Ergebnisse liefern können, sind sie nicht für historische Dokumente und oft auch nicht für andere Textgattungen gemacht. Deswegen gibt es in den Digital Humanities zahlreiche Bestrebungen, eigene Modelle speziell zur Named Entity Recognition verschiedener historischer Textkorpora zu entwickeln. Ein paar Beispiele: 

* https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.48
* https://arxiv.org/pdf/2205.15575.pdf 
* https://huggingface.co/dbmdz
* https://huggingface.co/hmbyt5-preliminary/byt5-small-historic-multilingual
* https://huggingface.co/hmbert/flair-hipe-2022-newseye-de

Die Übertragbarkeit dieser Modelle auf andere historische Perioden und Texte ist jedoch begrenzt und die Performance selbst dieser hochspezialisierten Modelle liegt momentan noch unter der für moderne Texte ([Ehrmann et al. 2023](https://dl.acm.org/doi/full/10.1145/3604931)). Historische Briefe und andere Texte, die besonders kurz und umgangssprachlich sind, stellen eine zusätzliche Herausforderung dar (z.B. [Jiang et al. 2022]( 	
https://doi.org/10.48550/arXiv.2201.07281
)) Named Entity Recognition für historische Texte bleibt also wohl auch in den nächsten Jahren ein Forschungsthema.  

Einen hervorragenden Überblick über Probleme und den Forschungsstand bei der Named Entity Recognition für historische Texte bieten: 

* Maud Ehrmann et al. (2023). Named Entity Recognition and Classification in Historical Documents: A Survey, in: ACM Computing Survey Vol. 56 / 2, pp. 1-47,  https://doi.org/10.1145/3604931. 

Eine etwas zugänglichere Diskussion zur Übertragbarkeit von Natural Language Processing Methoden wie NER auf historische Texte und Fragestellungen der Digital Humanities bieten: 

* Barbara McGillivray, Thierry Poibeau und Pablo Ruiz Fabo (2020). Digital Humanities and Natural Language Processing: "Je t'aime... Moi non plus", in: Digital Humanities Quarterly Vol 14 / 2, http://digitalhumanities.org:8081/dhq/vol/14/2/000454/000454.html.

Aber trotz dieser Schwierigkeiten ist Named Entity Recognition eine wichtige Methode: NER kann, sofern die Ergebnisse brauchbar sind, nicht nur zur Textanalyse verwendet werden, sondern zum Beispiel auch dazu, Dokumente im Rahmen einer digitalen Edition mit Metainformationen anzureichern und zu verknüpfen ([Beispiel 1](https://grandtourdig.hypotheses.org/949), [Beispiel 2](https://hal.sorbonne-universite.fr/hal-01396037/document)), oder aber auch zum Geomapping, also dem Hinzufügen von Metainformationen auf Karten ( [Beispiel](https://towardsdatascience.com/quick-guide-to-entity-recognition-and-geocoding-with-r-c0a915932895)).


## Quellen {-}

- Jurafsky, Daniel und Martin, James H. (2024), *Speech and Language Processing. Chapter 6: Vector Semantics and Embeddings*,  https://web.stanford.edu/~jurafsky/slp3/ed3bookfeb3_2024.pdf.
- Grimmer, Justin, Roberts, Margaret und Stewart, Brandon (2022). *Text as Data. Ch. 7: The Vector Space Model and Similarity Metrics*. 
- Stoltz, Dustin und Taylor, Marshall (2024). *Mapping Texts. Ch. 11: Extended Inductive*, insb. Word Embeddings: The First Generation, Word Embeddings: The Next Generation und Inductive Analysis with Word Embeddings. 
- Stoltz, Dustin und Taylor, Marshall (2024). *Mapping Texts. Ch. 6: From Text to Numbers*,  insb. Term Features. 
- Jurafsky, Daniel und Martin, James H. (2023), *Speech and Language Processing. Chapter 8.3: Named Entities and Named Entity Tagging*, https://web.stanford.edu/~jurafsky/slp3/8.pdf.
- Jurafsky, Daniel (2022). *Vector 6 Word2Vec*, https://www.youtube.com/watch?v=FHhKmf1iPvw.
- Jurafsky, Daniel (2022). *Vector 4 Cosine Similarity*, https://www.youtube.com/watch?v=FHhKmf1iPvw.
- Kriegel, Klaus (2020). Skript zur Vorlesung "Lineare Algebra für Informatik" im Sommersemester 2020 an der Freien Universität Berlin. 
- Manning, Chris und Socher, Richard (2017). *Vorlesung "GloVe: Global Vectors for Word Representation"*, Stanford University, https://www.youtube.com/watch?v=ASn7ExxLZws. 
- Selinvanov, Dmitriy (2020). *GloVe Word Embeddings*, https://text2vec.org/glove.html.
- Akbik, Alan (2020). *Vortrag "Das Flair Framework zur automatischen Analyse von Texten"*, Gesellschaft für Informatik,  https://www.youtube.com/watch?v=LqElF1eZ_ps. 
- Benoit, Kenneth und Matsuo, Akitaka (2023). *A Guide to Using Spacyr*, https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html. 
- Explosion AI, SpaCy Dokumentationsseiten, https://spacy.io/.
- Explosion AI, SpaCy Dokumentationsseiten: Models & Languages, https://spacy.io/usage/models. 
- Liao, David, FlaiR: An R Wrapper for Accessing Flair NLP Library, https://davidycliao.github.io/flaiR/ und https://github.com/davidycliao/flaiR?tab=readme-ov-file#performing-nlp-tasks-in-r. 
- Akbik, Alan, Flair Dokumentationsseiten, https://flairnlp.github.io/.
- Akbik, Alan, Flair Dokumentationsseiten: Flair Embeddings, https://flairnlp.github.io/docs/tutorial-embeddings/flair-embeddings. 
- Stanford Natural Language Prcessing Group (ed.). *CoreNLP. Using CoreNLP within Other Programming Languages and Packages: R (CRAN)*, https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran.
- Arnold, Taylor (2017). *A Tidy Data Model for Natural Language Processing Using cleanNLP*, in: The R Journal Vol. 9/2, pp. 248-267, https://arxiv.org/pdf/1703.09570.pdf.
- Arnold, Taylor. CleanNLP Documentation, https://statsmaths.github.io/cleanNLP/ und https://cran.r-project.org/web/packages/cleanNLP/. 
- Charles Univ. Prague (ed.). *LinPipe Design and Philosophy*, https://ufal.mff.cuni.cz/linpipe/design und https://github.com/ufal/linpipe. 
- Charles Univ. Prague (ed.). *NameTag 2 Models*, https://ufal.mff.cuni.cz/nametag/2/models.
- BNOSAC. Nametagger R Package, https://github.com/bnosac/nametagger. 
- Arnold, Taylor. CleanNLP: A Tidy Data Model for Natural Language Processing, https://statsmaths.github.io/cleanNLP/. 
- Stanford Natural Language Processing Group. Using CoreNLP within Other Programming Languages and Packages, https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran.
- Super User (2018). *A Comparison Between SpaCy and UDPipe for Natural Language Processing for R Users*, https://www.r-bloggers.com/2018/02/a-comparison-between-spacy-and-udpipe-for-natural-language-processing-for-r-users/
- The HashTag Magazine (2023). *Uncover the Subtleties of Language - Flair is the new Advanced AI*, https://hashtagmagazine.medium.com/uncover-the-subtleties-of-language-flair-is-the-new-advanced-ai-307e22218178. 
- CodeTrade (2023). *The Battle of the NLP Libraries: Flair vs SpaCy*, https://www.codetrade.io/blog/the-battle-of-the-nlp-libraries-flair-vs-spacy/. 
- Reticulate 1.34 Documentation: Python Version Configuration, https://rstudio.github.io/reticulate/articles/versions.html.
- Ehrmann, Maud et al. (2023). *Named Entity Recognition and Classification in Historical Documents: A Survey*, in: ACM Computing Survey Vol. 56 / 2, pp. 1-47,  https://doi.org/10.1145/3604931. 
- Bizon Monroc, Claire et al. (2022). *A Commprehensive Study of Open-source Libraries for Named Entity Recognition on Handwritten Historical Documents*, in: International Workshop on Document Analysis Systems, https://hal.science/hal-04056513. 
- Schweter, Stefan und Baiter, Johannes (2019). *Towards Robust Named Entity Recognition for Historic German*, in: Proceedings of the 4th Workshop on Representation Learning for NLP, pp. 96-103, https://aclanthology.org/W19-4312, und https://github.com/dbmdz/historic-ner.
- Jiang, Hang et al. (2022). *Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis*,  	
https://doi.org/10.48550/arXiv.2201.07281. 
- Hiippala, Tuomo (2021). *Applied Language Technology: NLP for the Humanities. Word Embeddings in SpaCy*, in: Proceeedings of the Fifth Workshop on Teaching NLP. Association of Computational Linguistics, pp. 46-48, https://applied-language-technology.mooc.fi/html/notebooks/part_iii/05_embeddings_continued.html und https://aclanthology.org/2021.teachingnlp-1.5/. 
- Hugging Face, Using SpaCy at Hugging Face, https://huggingface.co/docs/hub/spacy. 
- Hugging Face, Using Flair at Hugging Face, https://huggingface.co/docs/hub/flair.
- Flair, ner-multi, https://huggingface.co/flair/ner-multi.
- SpaCy, Multi-language, https://spacy.io/models/xx.
- Akbik, Alan, Blythe, Duncan und Vollgraf, Roland (2018). *Contextual String Embeddings for Sequence Labeling*, in: Proceedings of the 27th International Conference on Computational Linguistics, pp. 1638–1649, https://aclanthology.org/C18-1139/.
- Honnibal, Matthew (2018). *Spacy's Entity Recognition Model: Incremental Parsing with Bloom Embeddings & Residual CNNs*, https://spacy.io/universe/project/video-spacys-ner-model.
- Explosion AI, SpaCy Dokumentationsseiten: Library Architecture, https://spacy.io/api. 
- Applied Language Technology (2021). *Contextual Word Embeddings in SpaCy*, https://www.youtube.com/watch?v=fAeW1D37h90.
- Zeman, Daniel and Rosa, Rudolf (2021). *Contextual Word Embeddings*, https://www.youtube.com/watch?v=J9uSXZTW5Oc, und https://ufal.mff.cuni.cz/courses/npfl120#lectures. 
- Ghassemi, Mohammad (2021). *NLP Lecture 4(a) - Simple Word Embeddings*, https://www.youtube.com/watch?v=LGJSZCvBT3g  und https://github.com/deskool/nlp-class
- Herremans, Dorien (2022), Word Embeddings, https://www.youtube.com/watch?v=QldIe6N5-Lc. 
- Mei, Ted (2020). *From Static Embedding to Contextualized Embedding*, https://ted-mei.medium.com/from-static-embedding-to-contextualized-embedding-fe604886b2bc.
- Ethayarajh, Kawin (2020). *BERT, ELMo, & GPT-2: How Contextual are Contextualized Word Representations?*,  https://ai.stanford.edu/blog/contextual/.
- Chang, Ting-Yun und Chen, Yun-Nung (2019). *What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition*, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing, pp. 6064–6070, https://aclanthology.org/D19-1627/.