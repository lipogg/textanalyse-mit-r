```{r setup, include=FALSE}
options(width = 9999)
```

# Word Embeddings und NER

Bisher haben wir Worthäufigkeiten und das gemeinsame Vorkommen verschiedener Wörter in einem Korpus aus frequentistischer und probabilistischer Perspektive betrachtet. Um Worthäufigkeiten zu deuten und zu vergleichen, haben wir dabei untersucht, wie wahrscheinlich es ist, dass zwei Wörter in einem Korpus gemeinsam vorkommen. In diesem Kapitel untersuchen wir das gemeinsame Vorkommen von Wörtern in einem Korpus aus einer anderen mathematischen Perspektive, und zwar aus der Perspektive der linearen Algebra. Ausgangspunkt dieser Betrachtungsweise ist wieder die Feature Co-Occurrence Matrix (FCM). Aber die Zeilen und Spalten der Matrix werden jetzt nicht nur als Vektoren im Sinne von R aufgefasst, sondern als Vektoren im Sinne der linearen Algebra. Da jede Zeile der Matrix einem Wort im Korpus entspricht, beschreiben die Einträge in dem Wort-Vektor, wie oft die anderen Wörter im Korpus mit diesem Wort gemeinsam vorkommen. Wörter werden nach diesem Modell also durch ihren Kontext repräsentiert. 

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/jurafsky-martin_vector_representation.png")
```

Quelle: Jurafsky/Martin 2024, S. 113. [Hier](https://www.youtube.com/watch?v=xtD47WHNhxk) in Videoform.  

In die Sprache der linearen Algebra übersetzt (und geometrisch interpretiert) beschreibt jeder Wort-Vektor einen Punkt in einem mehrdimensionalen Raum. Dieser Raum hat nicht nur zwei oder drei Dimensionen, sondern hundert oder tausend. Diese höheren Dimensionen kann man sich geometrisch also nicht mehr vorstellen, aber es hilft, sich das Konzept für einen zweidimensionalen Raum zu veranschaulichen:

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("images/jurafsky-martin_vector_space.png")
```

Quelle: Jurafsky/Martin 2024, S. 113. [Hier](https://www.youtube.com/watch?v=xtD47WHNhxk) in Videoform. 

Abstrakt könnt ihr euch diesen Raum als einen "semantischen Raum" vorstellen, in dem alle Wörter in einem Korpus eine bestimmte Position haben, und deren Entfernung voneinander davon abhängt, wie viele Einträge in den Vektoren übereinstimmen. Diese Auffassung von Wörtern als Vektoren erlaubt es, die Ähnlichkeit von zwei Wörtern mithilfe von **Ähnlichkeitsmaßen** wie der **Kosinus-Ähnlichkeit (cosine similariy)** zu untersuchen ([hier](https://youtu.be/-_i9Cg81rXA) eine kurze Erläuterung im Videoformat). Dieses Maß geht davon aus, dass ähnliche Wörter in einem semantischen Raum nah beieinander liegen (Jurafsky 2022). "Ähnlichkeit" wird hier gemessen über die Richtung und den Winkel von zwei Vektoren zueinander: Wenn zwei Vektoren dieselbe Richtung haben und der Winkel zwsichen den Vektoren sehr klein sind, dann haben sie einen ähnlichen Kontext, und sind sich ähnlich.

## Word Embeddings

Die Kookkurrenzmatrizen, mit denen wir bisher gearbeitet haben, eignen sich jedoch noch nicht für eine solche Darstellung. Erinnert euch daran, dass die meisten Einträge in jeder Zeile Nullen sind, weil Wörter nur mit einer kleinen Anzahl anderer Wörter gemeinsam vorkommen, aber die Matrix alle Wörter aus dem Korpus enthält. Diese Art von Vektoren werden "lang und spärlich" (long and sparse) genannt. Das Rechnen mit solchen Vektoren ist sehr aufwändig, und deswegen werden die Wort-Vektoren meist in "kurze und dichte" (short and dense) Vektoren umgewandelt, bevor Ähnlichkeitsmaße bestimmt werden (mehr dazu [hier](https://www.youtube.com/watch?v=FHhKmf1iPvw)). Solche "kurzen und dichten" Vektoren, die den Kontext eines bestimmten Wortes repräsentieren, werden **Word Embeddings** genannt. Word Embeddings können entweder direkt für eine Analyse verwendet werden, oder sie werden als Zwischenschritt in verschiedenen Methoden des "Natural Language Processing" verwendet. Im Folgenden werden wir beispielhaft zunächst Word Embeddings selbst erstellen und danach eine Methode kennenlernen, welche Word Embeddings als Zwischenschritt verwendet: die sogenannte Named Entity Recognition. Streng genommen haben wir sogar bereits mit POS Tagging und Dependency Parsing zwei Methoden kennengelernt, für die im Rahmen von UDPipe ebenfalls Word Embeddings verwendet werden. 

In diesem Abschnitt betrachten wir Word Embeddings zunächst als Analysewerkzeug. In der Literatur wird Word Embeddings oft die Eigenschaft zugeschrieben, die Bedeutung von Wörtern darzustellen: 

>"Embeddings represent meaning by leveraging the distributional hypothesis of language, an idea that goes back to at least Wittgenstein (1953). The distributional hypothesis has been succinctly summarized by an oft-repreated quote in this literature: 'you shall know a word by the company it keeps' (Firth, 1957). It posits that we can learn something about the semantic meanings of words on the basis of the words that appear frequently in a small context window around the focal word." (Grimmer et al. 2022, S. 79)

Dabei kann die in diesem Sinne verstandene semantische Bedeutung eines Wortes auf **Type- oder auf Tokenebene** erfasst werden: Entweder, ein Wort wird als Type betrachtet. Dann ist der Kontext jedes Wort, das im gesamten Korpus in einem bestimmten Abstand ("context window") mit diesem Wort gemeinsam vorkommt. Oder ein Wort wird als Token betrachtet und der Kontext ist für jedes Vorkommen verschieden. Die erste Art der Word Embeddings auf Type-Ebene wird als **"Static Word Embeddings"** bezeichnet. Embeddings auf Token-Ebene werden als "Contextualized" oder **"Contextual Word Embeddings"** bezeichnet. Daneben gibt es noch **Character Embeddings**, die (wie auch im Rahmen von UDPipe) häufig mit Word Embeddings im engeren Sinne kombiniert werden. Character Embeddings repräsentieren den Kontext einzelner Zeichen, also zum Beispiel alle Buchstaben, mit denen der Buchstabe "a" in einem bestimmten Kontextfenster vorkommt. 

Mit unserer kleinen Beispielanalyse zu den Grimm'schen Märchen sind wir ja schon fertig, aber wir betrachten im Folgenden noch ein letztes Mal die mittlerweile vermutlich leidigen, aber dafür bereits vertrauten Märchen.

```{r eval=FALSE}
library(readtext)
library(quanteda)

# Märchen von 1857 und 1812/15 einlesen und Informationen aus dem Dateinamen extrahieren
maerchen_alle <- readtext("maerchen_alle/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Titel", "Jahr"), encoding = "UTF-8")

```

```{r echo=FALSE, results=FALSE, message=FALSE, warning=FALSE}
library(readtext)
library(quanteda)
maerchen_alle <- readtext("data/maerchen_alle/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Titel", "Jahr"), encoding = "UTF-8")
```

```{r}
maerchen_corpus <- corpus(maerchen_alle) %>%
  corpus_subset(Jahr == 1857)
```


<details>
<summary><b>Anwendungsbeispiele: Word Embeddings</b></summary>

- Nikhil Garg, Londa Schiebinger, Dan Jurafsky und James Zou (2018), Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, https://www.pnas.org/doi/10.1073/pnas.1720347115.
- Sandeep Soni, Lauren F. Klein und Jacob Eisenstein (2021). Abolitionist Networks. Modeling Language Change in Nineteenth-Century Activist Newspapers, https://doi.org/10.22148/001c.18841.

</details>

### Count-Based Word Embeddings mit Quanteda und PMI-Werten

Code adaptiert von Stoltz/Taylor 2024, Mapping Texts, S. 210-214.

```{r}
# Funktion cos_sim() aus Stoltz/Taylor 2024, S. 91. 
cos_sim <- function(A, B) {
  sum(A * B) / sqrt(sum(A^2) * sum(B^2))
}

# Funktion fcm_pmi() von Kohei Watanabe (s. Abschnitt "Pointwise Mutual Information")
fcm_pmi <- function(x) {
  m <- x@meta$object$margin
  x <- as(x, "TsparseMatrix") #"dgTMatrix"
  x@x <- log(x@x / (m[x@i + 1] * m[x@j + 1]) * sum(m))
  x@x[x@x < 0] <- 0
  as.fcm(x)
}
```

```{r warning=FALSE, message=FALSE}
library(quanteda)

# Sätze als Dokumente festlegen
maerchen_sentences <- corpus_reshape(maerchen_corpus, to="sentences")
maerchen_toks <- tokens(maerchen_sentences, remove_punct=TRUE) %>%
  tokens_remove(pattern = stopwords("de")) %>%
  tokens_remove(pattern = "dass") 

# seltene Kookkurrenzen herausfiltern
feats <- dfm(maerchen_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()
maerchen_toks <- tokens_select(maerchen_toks, feats, padding = FALSE)

# FCM erstellen und nach PPMI-Werten gewichten
maerchen_fcm <- fcm(maerchen_toks, context="document")
maerchen_pmi <- fcm_pmi(maerchen_fcm)
```
```{r}
# spärliche Matrix in dichte Matrix umwandeln
maerchen_svd <- svd(maerchen_pmi)
# saveRDS(maerchen_svd, "maerchen_svd.rds")
```

```{r}
# Dimensionen auswählen 
maerchen_svd <- maerchen_svd$v[, 1:100]  
# Zeilennamen wieder hinzufügen
rownames(maerchen_svd) <- rownames(maerchen_pmi)
```


```{r}

vec <- maerchen_svd["Königstochter", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Königssohn", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Frosch", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Fuchs", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Schloss", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["Tochter", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)

vec <- maerchen_svd["ging", , drop=FALSE]
similarities <- apply(maerchen_svd, 1, cos_sim, B = vec)
similarities %>%
  sort(decreasing = TRUE) %>%
  head(n=30)


```


<details>
<summary><b>Mathematischer Hintergrund: SVD </b></summary>

Folgt noch.

Für den mathematischen Hintergrund zu PMI verweise ich auf den Abschnitt [8.4.4.2 "Pointwise Mutual Information (PMI)"](https://lipogg.github.io/textanalyse-mit-r/textanalyse-iii-wortfrequenzanalysen.html#pointwise-mutual-information-pmi).

</details>
<details>
<summary><b>Mathematischer Hintergrund: Kosinusähnlichkeit </b></summary>

Folgt noch.

</details>



### Prediction-Based Static Word Embeddings mit Quanteda und GloVe 

Code nach https://quanteda.io/articles/pkgdown/replication/text2vec.html

```{r}
library(quanteda)
library(quanteda.textstats)
library(text2vec)

maerchen_toks <- tokens(maerchen_corpus)

feats <- dfm(maerchen_toks, verbose = TRUE) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()

maerchen_toks <- tokens_select(maerchen_toks, feats, padding = TRUE)
maerchen_fcm <- fcm(maerchen_toks, context = "window", count = "weighted", weights = 1 / (1:5), tri = TRUE)

glove <- GlobalVectors$new(rank = 50, x_max = 10)
wv_main <- glove$fit_transform(maerchen_fcm, n_iter = 10,
                               convergence_tol = 0.01, n_threads = 8)

wv_context <- glove$components
dim(wv_context)

word_vectors <- wv_main + t(wv_context) # sum up u and v vectors (column and row vectors), https://youtu.be/ASn7ExxLZws?si=W8MGBXKGDF3E0WVK&t=2519

vec <- word_vectors["Königstochter", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Königssohn", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Schloss", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

vec <- word_vectors["Wald", , drop = FALSE]
cossim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(vec),
                          method = "cosine")
head(sort(cossim[, 1], decreasing = TRUE), 10)

```

Ein Beispiel zu Contextual Word Embeddings folgt noch.

<details>
<summary><b>Mathematischer Hintergrund: GloVe </b></summary>

Folgt noch.

</details>

## Named Entity Recognition (NER)

Word Embeddings sind im Bereich Natural Language Processing oft nur ein Schritt in einer Reihe von Textverarbeitungsmethoden. Mithilfe von statischen und kontextuellen Word Embeddings (aber eher nicht mit unseren simplen count-based embeddings) können sogenannte "benannte Entitäten" in Texten identifiziert werden, also Wörter, die auf eine bestimmte Art von Entität verweisen: Personen, Orte, Organisationen sind die gängigsten Beispiele:

> "A **named entity** is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of **named entity recognition (NER)** is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: **PER** (person), **LOC** (location), **ORG** (organization), or **GPE** (geo-political entity). However, the term **named entity** is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices."
([Jurafsky/Martin 2023, Ch.8, S.6](https://web.stanford.edu/~jurafsky/slp3/8.pdf))

Ähnlich wie beim POS Tagging geht es also bei der Named Entity Recognition (NER) darum, bestimmte Tokens (oder auch Tokenkombinationen) automatisiert zu identifizieren und mit einem Tag oder Label zu versehen. Anders als beim POS Tagging ist das Ziel bei der NER aber, semantische Strukturen zu erkennen und nicht syntaktische. Named Entity Recognition Systeme machen sich zur Erkennung der Entitäten die Eigenschaft von Word Embeddings zunutze, dass Wörter, die in ähnlichen semantischen Kontexten vorkommen (z.B. Städte, Personen, ...), im mehrdimensionalen Raum enger beeinanderliegen als Wörter, deren Kontexte sehr verschiedenen sind. 

Im folgenden werden beispielhaft zwei der populärsten NER-Systeme vorgestellt: SpaCy und Flair. Beide sind eigentlich für Python entwickelt, können aber mithilfe von sogenannten "Wrappern" auch aus R heraus angewandt werden. Beide Systeme nutzen Word Embeddings in Kombination mit einer Reihe anderer Verarbeitungsschritte, um Entitäten zu erkennen: SpaCy nutzt statische Embeddings (zumindest für deutsche Texte) und FlaiR nutzt kontextuelle Embeddings (allerdings nicht mehr ELMo, sondern eigentlich Character Embeddings, also Vektoren, die den Kontext einzelner Buchstaben repräsentieren). Dabei ist wichtig zu beachten, dass die Embeddings bei der hier vorgestellten simplen Anwendung dieser Systeme nicht alle auf der Grundlage der Eingabetexte erstellt werden, sondern dass ähnlich wie beim POS-Tagging andere Textkorpora verwendet werden, um Word Embeddings zu generieren und ein Sprachmodell zu trainieren, das Named Entities erkennen kann. 

Als Beispiel dient uns diesmal ein sehr kleines Korpus von Briefen aus der Korrespondenz von Karl Marx und Friedrich Engels aus der [digitalen Edition der Marx-Engels-Gesamtausgabe](https://megadigital.bbaw.de).  


```{r}
library(readtext)
marx_test <- readtext(file="./data/marx_briefe/Marx_Engels_London_25-1-1868.txt")

marx_briefe <- readtext(file="./data/marx_briefe/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Von", "An", "Ort", "Datum"), encoding = "UTF-8")

```

<details>
<summary><b>Anwendungsbeispiele: NER</b></summary>

- Lisa Mendelman and Anna Mukamal (2021). The Generative Dissensus of Reading the Feminist Novel, 1995-2020:  A Computational Analysis of Interpretive Communities, https://doi.org/10.22148/001c.30009.
- Cécile Armand (2024). Shaping the Transnational Public Sphere in Republican China. Discourses and Practices of the Rotary Club in the Shanghai Press (1919-1949), https://journalofdigitalhistory.org/en/article/69Xry3ztPAk5 ([hier](https://journalofdigitalhistory.org/en/article/69Xry3ztPAk5?idx=94&layer=hermeneutics&lh=860&pidx=94&pl=narrative&y=85) ist der Abschnitt, in dem es um NER geht). 

</details>

## Beispiel mit SpaCy / Spacyr

Python Setup 

```{r eval=FALSE}
install.packages("spacyr")
library("spacyr")

spacy_install(version = "apple")
```

```{r echo = FALSE, warning=FALSE, message=FALSE}
library("spacyr")
reticulate::use_python("./nlp-env/bin/python")
reticulate::use_condaenv("nlp-env")

```

Sprachmodell herunterladen und initialisieren

```{r eval=FALSE}
spacy_download_langmodel("de_core_news_lg")
```
```{r warning=FALSE}
spacy_initialize(model = "de_core_news_lg")
```

Jetzt können wir beginnen: 

```{r}
results <- spacy_parse(marx_test, lemma = FALSE, entity = TRUE)
results_entities <- entity_extract(results)
results_entities # View(results_entities)
```


```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("images/ner_spacy_results.png")
```

Quelle: https://megadigital.bbaw.de/briefe/detail.xql?id=M0000533.


```{r}
# Spalte doc_id des readtext-Dataframes ist per Default Dateiname; das ersetzen wir durch das Datum 
marx_briefe$doc_id <- marx_briefe$Datum
```

```{r attr.output='style="max-height: 200px;"'}
results_briefe <- spacy_parse(marx_briefe, lemma = FALSE, entity = TRUE)

briefe_entities <- entity_extract(results_briefe)
briefe_entities

briefe_pers <- briefe_entities[briefe_entities$entity_type == "PER",]
briefe_pers
```

```{r warning=FALSE, message=FALSE}
library(ggplot2)

marx_pers_plot <- ggplot(briefe_pers, aes(x = doc_id, y = entity)) +
  geom_point(alpha=0.6) +
  theme(axis.text.y = element_text(size=3), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5)) + 
  labs(x = "Brief", y = "Person")

library(plotly)
ggplotly(marx_pers_plot)

```

```{r eval=FALSE}
spacy_finalize()

```

* Spacyr Dokumentationsseite: 
* Spacyr Tutorial: https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html
* Spacy Dokumentationsseiten: https://spacy.io/usage
* Spacy Sprachmodelle: https://spacy.io/usage/models und https://spacy.io/models/de#de_core_news_lg

## Beispiel mit Flair / FlaiR


```{r eval=FALSE}
install.packages("reticulate")
```

```{r eval=FALSE}
reticulate::use_python("/Path/to/python") # >= 3.10!
reticulate::py_config() # Einstellungen überprüfen 
```

```{r eval=FALSE}
install.packages("remotes")
remotes::install_github("davidycliao/flaiR") # force=TRUE
```


```{r warning=FALSE, message=FALSE}
library(flaiR)
```

```{r}
tagger_ner <- load_tagger_ner("de-ner-large")
results <- get_entities(text=marx_test$text, doc_ids=marx_test$doc_id, tagger_ner)
results

```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("images/ner_flair_results.png")
```

Quelle: https://megadigital.bbaw.de/briefe/detail.xql?id=M0000533.


```{r attr.output='style="max-height: 200px;"'}
results_briefe <- get_entities(text=marx_briefe$text, doc_ids=marx_briefe$Datum, tagger_ner)
results_briefe # View(results_briefe)

results_pers <- results_briefe[results_briefe$tag == "PER",]
results_pers
```

```{r warning=FALSE, message=FALSE}
library(ggplot2)

marx_pers_plot <- ggplot(results_pers, aes(x = doc_id, y = entity)) +
  geom_point(alpha=0.6) +
  theme(axis.text.y = element_text(size=3), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5)) + 
  labs(x = "Brief", y = "Person")

library(plotly)
ggplotly(marx_pers_plot)
```



```{r echo=FALSE}

spacy_finalize()

```

* FlaiR Dokumentationsseite: https://github.com/davidycliao/flaiR
* Flair Dokumentationsseiten: https://flairnlp.github.io/docs/intro und  https://github.com/flairNLP/flair
* Flair-Sprachmodelle: https://flairnlp.github.io/docs/tutorial-basics/tagging-entities#list-of-ner-models und  https://huggingface.co/flair/ner-german-large
* ConLL 2003 Datensatz: https://huggingface.co/datasets/conll2003 und https://www.clips.uantwerpen.be/conll2003/ner/

## Vergleich und Ausblick

Neben Spacyr und FlaiR gibt es noch weitere Pakete, die es erlauben, Named Entity Recognition in R durchzuführen. Dazu zählt zum Beispiel das Paket nametagger, das von demselben Personenkreis entwickelt wurde, die auch UDPipe entwickelt haben, und verschiedene Pakete, die es erlauben, das an der Universität Stanford entwickelte NER-System CoreNLP zu verwenden (laut [Dokumentationsseite](https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran) wird CoreNLP von den R Paketen [CleanNLP](https://github.com/statsmaths/cleanNLP), [NLP](https://cran.r-project.org/web/packages/NLP/) und [CoreNLP](https://cran.r-project.org/web/packages/coreNLP/) unterstützt). Das Paket CleanNLP  erlaubt, verschiedene Natural Language Processing Pipelines aus R heraus zu verwenden und diese in einem "Tidy" Datenformat abzubilden (mehr dazu [hier](https://arxiv.org/pdf/1703.09570.pdf)).

Nametagger unterstützt aktuell nur NER für englischsprachige Dokumente (siehe [diesen Beitrag](https://bookdown.org/morleyjamesweston/MCM4TM/named-entity-recognition.html#basic-machine-learning-approaches) für ein Anwendungsbeispiel), und während CoreNLP für moderne deutschsprachige Texte [sehr gute Ergebnisse erzielt](https://github.com/MaviccPRP/ger_ner_evals), wurde gezeigt, dass Flair für historsiche Dokumente bessere Ergebnisse erreichen kann (zum Beispiel [hier](https://hal.science/hal-04056513/document) und [hier](https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/44844/Holmes_washington_0250O_20778.pdf)).

Aber für die Out-of-the-Box NER Modelle gilt genauso wie für die Modelle für das POS Tagging und Dependency Parsing: Auch, wenn in Einzelfällen Modelle, die auf modernen (Zeitungs-)texten trainiert sind, bei der NER historischer Dokumente brauchbare Ergebnisse liefern können, sind sie nicht für historische Dokumente und oft auch nicht für andere Textgattungen gemacht. Deswegen gibt es in den Digital Humanities zahlreiche Bestrebungen, eigene Modelle speziell zur Named Entity Recognition verschiedener historischer Textkorpora zu entwickeln. Ein paar Beispiele: 

* https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.48
* https://arxiv.org/pdf/2205.15575.pdf 
* https://huggingface.co/dbmdz
* https://huggingface.co/hmbyt5-preliminary/byt5-small-historic-multilingual
* https://huggingface.co/hmbert/flair-hipe-2022-newseye-de

Die Übertragbarkeit dieser Modelle auf andere historische Perioden und Texte ist jedoch begrenzt und die Performance selbst dieser hochspezialisierten Modelle liegt momentan noch unter der für moderne Texte ([Ehrmann et al. 2023](https://dl.acm.org/doi/full/10.1145/3604931)). Historische Briefe und andere Texte, die besonders kurz und umgangssprachlich sind, stellen eine zusätzliche Herausforderung dar (z.B. [Jiang et al. 2022]( 	
https://doi.org/10.48550/arXiv.2201.07281
)) Named Entity Recognition für historische Texte bleibt also wohl auch in den nächsten Jahren ein Forschungsthema.  

Einen hervorragenden Überblick über Probleme und den Forschungsstand bei der Named Entity Recognition für historische Texte bieten: 

* Maud Ehrmann et al. (2023). Named Entity Recognition and Classification in Historical Documents: A Survey, in: ACM Computing Survey Vol. 56 / 2, pp. 1-47,  https://doi.org/10.1145/3604931. 

Eine etwas zugänglichere Diskussion zur Übertragbarkeit von Natural Language Processing Methoden wie NER auf historische Texte und Fragestellungen der Digital Humanities bieten: 

* Barbara McGillivray, Thierry Poibeau und Pablo Ruiz Fabo (2020). Digital Humanities and Natural Language Processing: "Je t'aime... Moi non plus", in: Digital Humanities Quarterly Vol 14 / 2, http://digitalhumanities.org:8081/dhq/vol/14/2/000454/000454.html.

Aber trotz dieser Schwierigkeiten ist Named Entity Recognition eine wichtige Methode: NER kann, sofern die Ergebnisse brauchbar sind, nicht nur zur Textanalyse verwendet werden, sondern zum Beispiel auch dazu, Dokumente im Rahmen einer digitalen Edition mit Metainformationen anzureichern und zu verknüpfen ([Beispiel 1](https://grandtourdig.hypotheses.org/949), [Beispiel 2](https://hal.sorbonne-universite.fr/hal-01396037/document)), oder aber auch zum Geomapping, also dem Hinzufügen von Metainformationen auf Karten ( [Beispiel](https://towardsdatascience.com/quick-guide-to-entity-recognition-and-geocoding-with-r-c0a915932895)).


## Quellen {-}

- Jurafsky, Daniel und Martin, James H. (2024), *Speech and Language Processing. Chapter 6: Vector Semantics and Embeddings*,  https://web.stanford.edu/~jurafsky/slp3/ed3bookfeb3_2024.pdf.
- Grimmer, Justin, Roberts, Margaret und Stewart, Brandon (2022). *Text as Data. Ch. 7: The Vector Space Model and Similarity Metrics*. 
- Stoltz, Dustin und Taylor, Marshall (2024). *Mapping Texts. Ch. 11: Extended Inductive*, insb. Word Embeddings: The First Generation, Word Embeddings: The Next Generation und Inductive Analysis with Word Embeddings. 
- Stoltz, Dustin und Taylor, Marshall (2024). *Mapping Texts. Ch. 6: From Text to Numbers*,  insb. Term Features. 
- Jurafsky, Daniel und Martin, James H. (2023), *Speech and Language Processing. Chapter 8.3: Named Entities and Named Entity Tagging*, https://web.stanford.edu/~jurafsky/slp3/8.pdf.
- Jurafsky, Daniel (2022). *Vector 6 Word2Vec*, https://www.youtube.com/watch?v=FHhKmf1iPvw.
- Jurafsky, Daniel (2022). *Vector 4 Cosine Similarity*, https://www.youtube.com/watch?v=FHhKmf1iPvw.
- Kriegel, Klaus (2020). Skript zur Vorlesung "Lineare Algebra für Informatik" im Sommersemester 2020 an der Freien Universität Berlin. 
- Manning, Chris und Socher, Richard (2017). *Vorlesung "GloVe: Global Vectors for Word Representation"*, Stanford University, https://www.youtube.com/watch?v=ASn7ExxLZws. 
- Selinvanov, Dmitriy (2020). *GloVe Word Embeddings*, https://text2vec.org/glove.html.
- Akbik, Alan (2020). *Vortrag "Das Flair Framework zur automatischen Analyse von Texten"*, Gesellschaft für Informatik,  https://www.youtube.com/watch?v=LqElF1eZ_ps. 
- Benoit, Kenneth und Matsuo, Akitaka (2023). *A Guide to Using Spacyr*, https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html. 
- Explosion AI, SpaCy Dokumentationsseiten, https://spacy.io/.
- Explosion AI, SpaCy Dokumentationsseiten: Models & Languages, https://spacy.io/usage/models. 
- Liao, David, FlaiR: An R Wrapper for Accessing Flair NLP Library, https://davidycliao.github.io/flaiR/ und https://github.com/davidycliao/flaiR?tab=readme-ov-file#performing-nlp-tasks-in-r. 
- Akbik, Alan, Flair Dokumentationsseiten, https://flairnlp.github.io/.
- Akbik, Alan, Flair Dokumentationsseiten: Flair Embeddings, https://flairnlp.github.io/docs/tutorial-embeddings/flair-embeddings. 
- Stanford Natural Language Prcessing Group (ed.). *CoreNLP. Using CoreNLP within Other Programming Languages and Packages: R (CRAN)*, https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran.
- Arnold, Taylor (2017). *A Tidy Data Model for Natural Language Processing Using cleanNLP*, in: The R Journal Vol. 9/2, pp. 248-267, https://arxiv.org/pdf/1703.09570.pdf.
- Arnold, Taylor. CleanNLP Documentation, https://statsmaths.github.io/cleanNLP/ und https://cran.r-project.org/web/packages/cleanNLP/. 
- Charles Univ. Prague (ed.). *LinPipe Design and Philosophy*, https://ufal.mff.cuni.cz/linpipe/design und https://github.com/ufal/linpipe. 
- Charles Univ. Prague (ed.). *NameTag 2 Models*, https://ufal.mff.cuni.cz/nametag/2/models.
- BNOSAC. Nametagger R Package, https://github.com/bnosac/nametagger. 
- Arnold, Taylor. CleanNLP: A Tidy Data Model for Natural Language Processing, https://statsmaths.github.io/cleanNLP/. 
- Stanford Natural Language Processing Group. Using CoreNLP within Other Programming Languages and Packages, https://stanfordnlp.github.io/CoreNLP/other-languages.html#r-cran.
- Super User (2018). *A Comparison Between SpaCy and UDPipe for Natural Language Processing for R Users*, https://www.r-bloggers.com/2018/02/a-comparison-between-spacy-and-udpipe-for-natural-language-processing-for-r-users/
- The HashTag Magazine (2023). *Uncover the Subtleties of Language - Flair is the new Advanced AI*, https://hashtagmagazine.medium.com/uncover-the-subtleties-of-language-flair-is-the-new-advanced-ai-307e22218178. 
- CodeTrade (2023). *The Battle of the NLP Libraries: Flair vs SpaCy*, https://www.codetrade.io/blog/the-battle-of-the-nlp-libraries-flair-vs-spacy/. 
- Reticulate 1.34 Documentation: Python Version Configuration, https://rstudio.github.io/reticulate/articles/versions.html.
- Ehrmann, Maud et al. (2023). *Named Entity Recognition and Classification in Historical Documents: A Survey*, in: ACM Computing Survey Vol. 56 / 2, pp. 1-47,  https://doi.org/10.1145/3604931. 
- Bizon Monroc, Claire et al. (2022). *A Commprehensive Study of Open-source Libraries for Named Entity Recognition on Handwritten Historical Documents*, in: International Workshop on Document Analysis Systems, https://hal.science/hal-04056513. 
- Schweter, Stefan und Baiter, Johannes (2019). *Towards Robust Named Entity Recognition for Historic German*, in: Proceedings of the 4th Workshop on Representation Learning for NLP, pp. 96-103, https://aclanthology.org/W19-4312, und https://github.com/dbmdz/historic-ner.
- Jiang, Hang et al. (2022). *Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis*,  	
https://doi.org/10.48550/arXiv.2201.07281. 
- Hiippala, Tuomo (2021). *Applied Language Technology: NLP for the Humanities. Word Embeddings in SpaCy*, in: Proceeedings of the Fifth Workshop on Teaching NLP. Association of Computational Linguistics, pp. 46-48, https://applied-language-technology.mooc.fi/html/notebooks/part_iii/05_embeddings_continued.html und https://aclanthology.org/2021.teachingnlp-1.5/. 
- Hugging Face, Using SpaCy at Hugging Face, https://huggingface.co/docs/hub/spacy. 
- Hugging Face, Using Flair at Hugging Face, https://huggingface.co/docs/hub/flair.
- Flair, ner-multi, https://huggingface.co/flair/ner-multi.
- SpaCy, Multi-language, https://spacy.io/models/xx.
- Akbik, Alan, Blythe, Duncan und Vollgraf, Roland (2018). *Contextual String Embeddings for Sequence Labeling*, in: Proceedings of the 27th International Conference on Computational Linguistics, pp. 1638–1649, https://aclanthology.org/C18-1139/.
- Honnibal, Matthew (2018). *Spacy's Entity Recognition Model: Incremental Parsing with Bloom Embeddings & Residual CNNs*, https://spacy.io/universe/project/video-spacys-ner-model.
- Explosion AI, SpaCy Dokumentationsseiten: Library Architecture, https://spacy.io/api. 
- Applied Language Technology (2021). *Contextual Word Embeddings in SpaCy*, https://www.youtube.com/watch?v=fAeW1D37h90.
- Zeman, Daniel and Rosa, Rudolf (2021). *Contextual Word Embeddings*, https://www.youtube.com/watch?v=J9uSXZTW5Oc, und https://ufal.mff.cuni.cz/courses/npfl120#lectures. 
- Ghassemi, Mohammad (2021). *NLP Lecture 4(a) - Simple Word Embeddings*, https://www.youtube.com/watch?v=LGJSZCvBT3g  und https://github.com/deskool/nlp-class
- Herremans, Dorien (2022), Word Embeddings, https://www.youtube.com/watch?v=QldIe6N5-Lc. 
- Mei, Ted (2020). *From Static Embedding to Contextualized Embedding*, https://ted-mei.medium.com/from-static-embedding-to-contextualized-embedding-fe604886b2bc.
- Ethayarajh, Kawin (2020). *BERT, ELMo, & GPT-2: How Contextual are Contextualized Word Representations?*,  https://ai.stanford.edu/blog/contextual/.
- Chang, Ting-Yun und Chen, Yun-Nung (2019). *What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition*, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing, pp. 6064–6070, https://aclanthology.org/D19-1627/.