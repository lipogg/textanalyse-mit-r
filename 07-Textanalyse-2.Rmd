# Textanalyse II: Preprocessing

Im Kapitel "Textanalyse I" haben wir bereits die grundlegenden Quanteda-Objekte kennengelernt: Korpus-Objekte, Tokens-Objekte und DFM-Objekte. Bei der Erstellung des Tokens-Objekts wurden die Texte tokenisiert, also in einzelne Tokens zerlegt. Vor und/oder nach der Tokenisierung erfolgen häufig noch weitere Operationen zur Bereinigung und Vorbereitung der Texte, die wir im folgenden kennenlernen werden. Solche Operationen zur Vorbereitung und Bereinigung von Texten zur Analyse werden allgemein **Preprocessing** genannt. Dazu gehören z.B. das Entfernen von Satzzeichen und von bestimmten Wörtern, die sehr häufig vorkommen (sogenannte "Stoppwörter"), die Umwandlung aller Tokens in Kleinbuchstaben, die sogenannte "Lemmatisierung" der Texte oder die manuelle Bereinigung einzelner Tokens mithilfe von speziellen Ausdrücken, die sich "Reguläre Ausdrücke" (oder engl. "Regular Expressions") nennen. Im Folgenden schauen wir uns diese und einige weitere Preprocessing-Schritte am Beispiel unseres Korpus deutschsprachiger belletristischer Texte an. 

Welche Preprocessing-Schritte im Einzelnen durchgeführt werden, hängt vom Kontext, von der Qualität der Texte und von der Forschungsfrage ab. Für manche Forschungsfragen kann es z.B. interessant sein, manche Stoppwörter beizubehalten oder zusätzliche Wörter zu entfernen. In anderen Fällen soll dagegen vielleicht mit den Grundformen der Wörter (Lemma) gearbeitet werden; die Texte müssen also "lemmatisiert" werden. 


:::tip
Der Pipe-Operator `%>%`

In den Beispielen in diesem Kapitel kommt manchmal der sogenannte Pipe-Operator `%>%` vor. Diesen Operator habt ihr bereits im Kapitel ["R Basics IV: Funktionen und Pakete"](https://lipogg.github.io/textanalyse-mit-r/r-basics-iv-funktionen-und-pakete.html#wozu-werden-pakete-verwendet) und in den Übungsaufgaben kurz kennengelernt. Zur Erinnerung: Der Pipe-Operator wird verwendet, um mehrere Funktionsaufrufe miteinander zu verketten. Dabei übernimmt die nachfolgende Funktion als erstes Argument jeweils den Rückgabewert der vorhergehenden Funktion. Im folgenden Beispiel übergibt der Pipe-Operator der Funktion `paste0()` das Objekt `greeting` als Argument. Die `paste0()`-Funktion fügt an die Begrüßung ein Ausrufezeichen an und übergibt die bearbeitete Zeichenkette "Guten Tag!" an die Funktion `strsplit()`. Die Funktion `strsplit()` teilt dann den Satz anhand der Leerzeichen in einzelne Einheiten auf und gibt einen character-Vektor zurück. Dieser character-Vektor wird zuletzt der Variable `greeting_toks` zugewiesen. 

```
greeting <- "Guten Tag"

greeting_toks <- greeting %>%
  paste0("!") %>%
  strsplit(" ")

```

Ein Ausdruck der Art `x %>% f` ist also äquivalent zu `f(x)`.

:::




## Tokenisieren und segmentieren

Das Tokenisieren, also das Zerlegen von Zeichenketten in Tokens, haben wir schon kennengelernt. Wir schauen uns als Beispiel wieder den Beispielsatz aus dem letzten Übungsblatt an, mit ein paar Zusätzen:

```{r warning=FALSE, message=FALSE}
library(quanteda)

beispiel_1 <- "Hallo mein Name ist Mr. Robert De Niro und das ist meine Telefonnummer: 0164-452954322. Meine E-Mail-Adresse ist niro@gmail.com und ich bin geboren am 02-04-1965. #callme"

beispiel_toks <- tokens(beispiel_1)
print(beispiel_toks, max_ntoken = 200)
```

In dem Beispielsatz werden alle Sinneinheiten richtig als Tokens erkannt. Aber was passiert, wenn z.B. die Telefonnummer und das Geburtsdatum etwas anders aussehen und anstelle eines Trennstrichs ein Schrägstrich verwendet wird? 

```{r warning=FALSE, message=FALSE}
beispiel_2 <- "Hallo mein Name ist Mr. Robert De Niro und das ist meine Telefonnummer: 0164/452954322. Meine E-Mail-Adresse ist niro@gmail.com und ich bin geboren am 02/04/1965. #callme"

beispiel_toks <- tokens(beispiel_2)

print(beispiel_toks, max_ntoken = 200)

```

Dann werden die Telefonnummer und das Geburtsdatum nicht mehr als Sinneinheiten erkannt. In diesem Fall gibt es zwei Möglichkeiten: Entweder die Tokenisierungsregeln werden angepasst, oder die Tokens, die falsch erkannt werden, werden vor dem Tokenisieren so bearbeitet, dass sie nach den bestehenden Tokenisierungsregeln als Sinneinheit erkannt werden. Die manuelle Anpassung der Tokenisierungsregeln ist recht komplex und würde den Rahmen etwas sprengen; ihr könnt allerdings bei Interesse  [hier](https://quanteda.io/reference/tokenize_custom.html) nachlesen, wie das geht. Wir schauen uns stattdessen an, wie die Tokens so bearbeitet werden können, dass sie richtig erkannt werden. In unserem Beispiel geht das ganz einfach mit der R-Basisfunktion `gsub()`: 

```{r warning=FALSE, message=FALSE}
beispiel_2 <- gsub("/", "-", beispiel_2)
beispiel_toks <- tokens(beispiel_2) 
print(beispiel_toks, max_ntoken = 200)
```

Wenn dagegen aus irgendeinem Grund die Telefonnummer und das Geburtsdatum nicht als Sinneinheit behandelt werden sollen, sondern die Zeichen jeweils eigene Tokens bilden sollen, kann dagegen einfach beim Aufruf der `tokens()`-Funktion das zusätzliche Argument `split_hyphens` übergeben werden:

```{r warning=FALSE, message=FALSE}
beispiel_toks <- tokens(beispiel_1, split_tags = TRUE, split_hyphens = TRUE )
print(beispiel_toks, max_ntoken = 200)
```

Zeichenketten können nicht nur wie bisher in Wörter und andere Äußerungen zerlegt werden. Manchmal ist es sinnvoll, Texte in kleinere oder größere Einheiten zu zerlegen, also z.B. in einzelne Zeichen oder einzelne Sätze. Wenn Texte in größere Segmente wie Sätze zerlegt werden, nennt man diese Operation **Segmentieren** (oder engl. "segmentation"). Dazu kann ebenfalls die Funktion `tokens()` verwendet werden, mit dem zusätzlichen Argument `what = "character"` bzw. `what = "sentence"`: 

```{r warning=FALSE, message=FALSE}
# Segmentieren auf Satzebene
beispiel_segments <- tokens(beispiel_1, what = "sentence")
print(beispiel_segments, max_ntoken = 200)
# Tokenisieren auf Zeichenebene
beispiel_chars <-  tokens(beispiel_1, what = "character")
print(beispiel_chars, max_ntoken = 200)
```

Da Tokenisieren eine komplexe Operation ist, dauert es je nach Anzahl und Länge der Texte lange, bis der Computer ein komplettes Korpus tokenisiert hat. Wenn die  `tokens()`-Funktion mit dem zusätzlichen Argument `verbose = TRUE` aufgerufen wird, werden beim Ausführen der Funktion Updates zu jedem Bearbeitungsschritt auf der Konsole ausgegeben: 

```{r warning=FALSE, message=FALSE}
tokens(beispiel_1, verbose = TRUE)
```

Die `tokens()`-Funktion kann auch zum Tokenisieren von japanisch- und chinesischsprachigen Texten verwendet werden. Hierbei wird unter der Motorhaube eine morphologische Analyse durchgeführt. Ein Beispiel aus den [Quanteda-Dokumentationsseiten](https://tutorials.quanteda.io/multilingual/overview/): 

```{r warning=FALSE, message=FALSE}
library(readtext)
declaration <- readtext("data/declaration_rights.txt")
cor <- corpus(declaration)
toks <- tokens(cor)
toks
```

:::task
Verständnisfragen: 

* Im Abschnitt 5.2 "Korpus, Tokens und Types" haben wir uns [zwei Tweets von Greta Thunberg und Andrew Tate](https://x.com/GretaThunberg/status/1608056944501178368) angesehen und inhaltlich zusammenhängende Einheiten identifiziert. Kopiert die beiden Tweets in R und tokenisiert sie mithilfe der Quanteda-Funktion tokens(). Werden alle Sinneinheiten richtig erkannt?

:::

## Reguläre Ausdrücke im Preprocessing

Manchmal ist es notwendig, eine Zeichenkette vor dem Tokenisieren manuell zu bearbeiten oder bereinigen, damit beim Preprocessing die Tokens für den jeweiligen Kontext richtig erkannt werden. Wir haben zum Beispiel gesehen, dass beim Tokenisieren manche Sinneinheiten richtig erfasst werden (z.B. Hashtags oder Telefonnummern mit `-`), aber andere nicht (z.B. Telefonnummern mit `/`, der Punkt nach einer Abkürzung wie Mr., der Nachname De Niro). Um eines dieser Probleme zu beheben, haben wir den Text manuell bearbeitet und mithilfe der Funktion `gsub()` alle Schrägstriche gegen Trennstriche ausgetauscht. Es kommt daneben auch vor, dass Texte bestimmte Zeichen enthalten, die keine inhaltliche Bedeutung tragen, zum Beispiel Fußnoten oder Seitenzahlen. Solche Zeichen können die Ergebnisse der Textanalyse beeinflussen und sollten deswegen im Rahmen des Preprocessing entfernt werden. Zur Suche, zum Bearbeiten und zur Entfernung von Zeichen in Zeichenketten können reguläre Ausdrücke (engl. "regular expressions") verwendet werden. Eine ausführliche Einführung in reguläre Ausdrücke findet ihr im Kapitel "Exkurs: Reguläre Ausdrücke". In diesem Abschnitt schauen wir uns nur an einem Beipsiel an, wie reguläre Ausdrücke beim Preprocessing zur Anwendung kommen können.  

Der Beispieltext `froschkoenig` enthält Verweise auf Fußnoten in eckigen Klammern. Diese Verweise wollen wir nun entfernen. Die bereits bekannte Funktion `gsub()` kann verwendet werden, um mithilfe von regulären Ausdrücken Muster zu definieren, die in einer Zeichenkette ausgetauscht werden sollen. Um alle Verweise zu entfernen, definieren wir einen regulären Ausdruck, der nach einem Leerzeichen gefolgt von mehr als einer (`+`) Zahl zwischen 0 und 9 (`[0-9]`) innerhalb von eckigen Klammern (`\\[` oder `\\]`) sucht und durch einen leeren String (`""`) austauscht. Bevor wir die Seitenzahlen entfernen, sollten wir uns allerdings die Suchergebnisse anzeigen lassen, um zu überprüfen, ob der reguläre Ausdruck die richtigen Zeichenketten findet:  

```{r}
froschkoenig <- "In den alten Zeiten [1], wo das Wünschen noch geholfen hat, lebte ein König [2], dessen Töchter waren alle schön, aber die jüngste Tochter [3] war so schön, daß die Sonne selber, die doch so vieles gesehen hat, sich verwunderte so oft sie ihr ins Gesicht schien."

regmatches(froschkoenig, gregexpr(" \\[[0-9]+\\]", froschkoenig))

froschkoenig <- gsub(" \\[[0-9]+\\]", "", froschkoenig)
froschkoenig
```


Die eckige Klammer steht in einem Regex-Ausdruck für Zeichenklassen (s. "Exkurs: Reguläre Ausdrücke"). Die doppelten \\\\ werden verwendet, damit die eckige Klammer nicht als Regex-Symbol erkannt wird, sondern als ganz normales Satzzeichen. 

:::task
Verständnisfragen: 

* Was passiert, wenn man die \\\\ weglässt?
* Was passiert, wenn man das Leerzeichen am Anfang des regulären Ausdrucks `" \\[[1-9]+\\]"` weglässt?
* Was machen die Funktionen `gregexpr()` und `regmatches()`?
* Wie könnte man die beiden Tweets aus der Verständnisfrage zum vorigen Abschnitt bearbeiten, damit "quad turbo" als Sinneinheit erkannt wird?

:::


## Satzzeichen, Zahlen und Sonderzeichen entfernen 

Für viele Analysemethoden spielen nur Wörter im eigentlichen Sinne eine Rolle. Satzzeichen, Zahlen und Sonderzeichen werden deswegen häufig beim Preprocessing entfernt. Dieser Vorbereitungsschritt ist so verbreitet, dass die Entwickler:innen des Pakets quanteda Parameter für die `tokens()`-Funktion definiert haben, die steuern, ob bei der Tokenisierung diese Zeichen direkt entfernt werden sollen oder nicht.  

```{r}
froschkoenig_toks <- tokens(froschkoenig, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
froschkoenig_toks
```


## Stoppwörter entfernen 

Wenn Texte zur Analyse als "Bag of Words" repräsentiert werden sollen, dann spielt die Reihenfolge der einzelnen Worte keine Rolle und Funktionswörter wie Artikel, Konjunktionen, Präpositionen u.ä. sind außerhalb des Satzzusammenhangs für diese Analysemethoden häufig nicht von Interesse. Zugleich kommen diese Wörter aber deutlich häufiger vor als "bedeutungstragende" Wörter. Solche Wörter werden deswegen häufig beim Preprocessing entfernt. Dabei werden sogenannte Stoppwortlisten verwendet: Alle Tokens, die in der Liste vorkommen, werden nach dem Tokenisieren entfernt. Die Funktionen zum Tokenisieren und zum Entfernen der Stoppwörter kann in Quanteda mithilfe des Pipe-Operators verkettet werden:

```{r}
# Funktion tokens_remove() zum entfernen der Stoppwörter verwenden 
froschkoenig_toks <- tokens(froschkoenig) %>%
  tokens_remove(pattern = stopwords("de"))
print(froschkoenig_toks, max_ntoken = 200)

```

Beim Aufruf der Funktion `tokens_remove()` wird mithilfe des Arguments `pattern = stopwords("de")` eine Stoppwortliste mit deutschen Stoppwörtern übergeben. Welche Wörter die  `tokens_remove()`-Funktion entfernt, hängt also davon ab, welche Stoppwörter auf dieser Liste stehen. Der [Funktionsdokumentation](https://quanteda.io/reference/stopwords.html) können wir entnehmen, dass die Stoppwortliste einem weiteren R Paket entnommen wird. Die Liste deutscher Stoppwörter ist demzufolge: 
http://snowball.tartarus.org/algorithms/german/stop.txt

Eine Stoppwortliste ist also im Grunde nur eine Plaintextdatei, in der in jeder Zeile ein Wort steht. Anstelle der vordefinierten Stoppwortliste kann auch eine eigene Stoppwortliste eingelesen werden. Dazu kann entweder eine Liste komplett selbst erstellt werden, oder es wird zunächst eine Stoppwortliste heruntergeladen und dann angepasst, zum Beispiel: 

* https://github.com/solariz/german_stopwords
* https://github.com/stopwords-iso/stopwords-de/blob/master/stopwords-de.txt

Die Liste kann angepasst werden, indem einfach Wörter in der Plaintext-Datei hinzugefügt oder entfernt werden. 

```{r eval=FALSE}

# Eigene Stoppwortliste einlesen
custom_stopwords <- readLines("stopwords.txt", encoding = "UTF-8") 
custom_stopwords <- readtext("stopwords.txt") 
custom_stopwords
# Importierte Stoppwortliste and die tokens_remove()-Funktion übergeben
froschkoenig_toks <- tokens_remove(froschkoenig_toks, pattern = custom_stopwords, padding=F)

```

Alternativ kann auch die Default-Stoppwortliste der Funktion `stopwords()` durch eine andere Stoppwortliste ausgetauscht werden. Um zu überprüfen, welche Stoppwortlisten es gibt: 

```{r}
library(stopwords)
stopwords_getsources()
froschkoenig_toks <- tokens(froschkoenig) %>%
  tokens_remove(pattern = stopwords("de", source="nltk"))

```

Zuletzt können mit der `tokens_remove()`-Funktion auch nachträglich einzelne Tokens entfernt werden, die in einem Text vielleicht besonders häufig vorkommen, aber nicht auf der Stoppwortliste stehen: 

```{r}
froschkoenig_toks <- tokens_remove(froschkoenig_toks, pattern = "daß", padding=F)

```

## Groß- und Kleinschreibung anpassen

Wir haben bereits gesehen, dass dasselbe Wort groß- und kleingeschrieben als zwei verschiedene Types gezählt wird. Dieses Verhalten ist bei der Analyse oft nicht gewünscht, da die unterschiedliche Schreibweise meist nicht als inhaltlich bedeutungstragend angesehen wird. Beim Preprocessing kann deswegen zusätzlich der gesamte Text in Kleinbuchstaben umgewandelt werden: 

```{r}
froschkoenig_toks <- froschkoenig %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% 
  tokens_tolower()
froschkoenig_toks
```

## Stemming 

> "a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word" 

Quelle: [Jurafsky/Martin 2023, S. 2](https://web.stanford.edu/~jurafsky/slp3/2.pdf)

Beim Stemming werden Wörter auf ihren Wortstamm reduziert, indem Wortendungen nach bestimmten Regeln entfernt werden. Das Stemming schauen wir uns nur äußerst kurz an, denn in der Praxis lohnt es sich selten, dieses Verfahren anzuwenden. Zum Stemming kann die quanteda-Funktion `tokens_wordstem()` verwendet werden: 

```{r}
library(quanteda)

beispiel <- "Hallo mein Name ist Mr. Robert De Niro und das ist meine Telefonnummer: 0164-452954322. Meine E-Mail-Adresse ist niro@gmail.com und ich bin geboren am 02/04/1965. #callme"

beispiel_stem <- tokens_wordstem(tokens(beispiel), language="ger")
print(beispiel_stem, max_ntoken = 200)

```


## Lemmatisierung

 > "the task of determining that two words have the same root, despite their surface differences" 
 
Quelle: [Jurafsky/Martin 2023, S. 2](https://web.stanford.edu/~jurafsky/slp3/2.pdf)

Lemmatisierung (engl. lemmatization) hat im Grunde dasselbe Ziel wie Stemming: Bei der Lemmatisierung werden Wörter auf ihre Grundform reduziert. Dazu werden jedoch komplexere Algorithmen angewandt, sodass auch Grundformen erkannt werden, die durch die einfache Entfernung von Endungen nicht richtig gebildet würden.

### Methode 1: Lemmatisierung mit Lexikon

Diese Methode kann zur Lemmatisierung englischsprachiger Texte angewandt werden.

```{r eval=FALSE}
install.packages("lexicon")
```

```{r warning=FALSE, message=FALSE}
library(lexicon)
```

```{r eval = FALSE}
beispiel_engl <- "Hello I went swimming yesterday"

beispiel_lemmatized_2 <- tokens_replace(tokens(beispiel_engl), pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma)
print(beispiel_lemmatized_2, max_ntoken = 200)

# dasselbe geht auch mit der Funktion dfm_replace()
```


### Methode 2: Lemmatisierung mit UDPipe 

```{r setup, include=FALSE}
options(width = 9999)
```


Diese Methode kann zur Lemmatisierung auch von deutsch- und anderssprachigen Texten angewandt werden. Das Verfahren ist ausführlich dokumentiert in den UDPipe-Dokumentationsseiten: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html. In der Sitzung zum fortgeschrittenen Preprocessing werden wir noch einmal darauf zurückkommen und erarbeiten uns in diesem Abschnitt nur ein grundlegendes Verständnis der Lemmatisierung mithilfe des R Pakets `udpipe`. Zunächst muss das Paket `udpipe` installiert und geladen werden. Anschließend muss ein sogenanntes Sprachmodell heruntergeladen und eingelesen werden. Die `udpipe`-Sprachmodelle sind statistische Modelle, die auf einem bestimmten Datensatz "trainiert" wurden, also zum Beispiel auf einem Korpus deutschsprachiger Texte, die mit linguistischen Annotationen versehen wurden. Auf der Grundlage der Trainingsdaten können danach Bestandteile wie Wörter und Satzzeichen auch in unbekannten Texten erkannt werden: Zum Beispiel Wortarten, Grundformen und syntaktische Beziehungen. Genau das passiert, wenn wir einer udpipe-Funktion wie `udpipe()` oder `udpipe_annotate()` auf unser Beispielkorpus anwenden. 


```{r eval=FALSE}
install.packages("udpipe")
library(udpipe)

# Deutsches Sprachmodell herunterladen und laden
ud_model <- udpipe_download_model("german")
ud_model <- udpipe_load_model(ud_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE,}
library(udpipe)
ud_model <- udpipe_load_model("./models/german-gsd-ud-2.5-191206.udpipe")

```

Es gibt zwei Funktionen aus dem Paket udpipe, mit denen Texte lemmatisiert werden können, also mit denen die Wörter in einem Text auf ihre Grundformen reduziert werden können. Die Funktion `udpipe()` gibt direkt einen Dataframe zurück (s.u.). Die Funktion `udpipe_annotate()` gibt eine Liste zurück, die in einem folgenden Schritt in einen Dataframe umgewandelt werden kann. Beide Funktionen lemmatisieren den Text nicht nur, sondern tokenisieren ihn auch und führen weitere Verarbeitungsschritte durch, auf die wir an dieser Stelle nicht weiter eingehen. Die Funktion `udpipe_annotate()` erlaubt es, mithilfe verschiedener Funktionsparameter festzulegen, welche dieser Verarbeitungsschritte beim Aufruf der Funktion durchgeführt werden sollen. 

Wir betrachten zunächst wieder unseren Beispielsatz: 

```{r}

beispiel <- "Hallo mein Name ist Mr. Robert De Niro und das ist meine Telefonnummer: 0164-452954322. Meine E-Mail-Adresse ist niro@gmail.com und ich bin geboren am 02/04/1965. #callme"
```

```{r attr.output='style="max-height: 200px;"'}
beispiel_df <- udpipe(beispiel, ud_model)
head(beispiel_df) # erste fünf Zeilen des Dataframes anzeigen 
beispiel_annotated <- udpipe_annotate(ud_model, beispiel, tagger="default", parser="none")
beispiel_df <- as.data.frame(beispiel_annotated)
head(beispiel_df) # erste fünf Zeilen des Dataframes anzeigen
#View(beispiel_df)
# ?udpipe_annotate

```

Beachtet Zeile 29: Hier wurde das Token "am" in zwei Lemmata aufgeteilt: "an" und "der". Dieses Verhalten müssen wir bei der Weiterverarbeitung der Lemmata beachten!

Jetzt schauen wir uns an, wie nicht nur ein einziger Text, sondern ein ganzes Korpus mithilfe von udpipe lemmatisiert werden kann. Als Beispiel dient uns das Teilkorpus mit Kafka-Texten aus der letzten Stunde. Dazu erstellen wir, analog zur letzten Stunde, zunächst ein Teilkorpus aus Kafka-Texten und Tokenisieren das Korpus: 

```{r eval=FALSE}
library(readtext)
library(quanteda)

ger_texte <- readtext("korpus/*.txt", docvarsfrom = "filenames", dvsep = "_", docvarnames = c("Autor_in", "Titel", "Jahr"), encoding = "UTF-8")
ger_korpus <- corpus(ger_texte)
kafka_korpus <- corpus_subset(ger_korpus, Autor_in == "kafka")
kafka_toks <- tokens(kafka_korpus)

```

Anschließend können wir die Funktion `udpipe_annotate()` auf unser Kafka-Korpus anwenden. Dazu muss jedoch erst das Quanteda-Tokens-Objekt in eine Form gebracht werden, den die udpipe_annotate()-Funktion als Argument annehmen kann. Den Code dafür entnehmen wir dieser [Anleitung von Jan Wijffels](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html), einem der Entwickler:innen von UDPipe. 

```{r eval = FALSE}
library(udpipe)

# Put every token on a new line and use tokenizer: vertical, Code von Jan Wijffels
kafka_txt <- sapply(kafka_toks, FUN=function(x) paste(x, collapse = "\n"))
kafka_annotated <- udpipe_annotate(ud_model, kafka_txt, tagger="default", parser="none", doc_id = kafka_toks$Titel, tokenizer = "vertical", trace=TRUE)
kafka_df <- as.data.frame(kafka_annotated)
head(kafka_df, n=100)
```


```{r attr.output='style="max-height: 200px;"', echo = FALSE}
kafka_df <- readRDS("data/kafka_df.rds")
head(kafka_df, n=100)
```

Da bei der Lemmatisierung Tokens wie "am" in zwei Lemmata aufgeteilt werden ("an" und "dem"), gibt es im Dataframe `kafka_df` einige Zeilen, in denen in der Spalte `lemma` der Wert `NA` steht. Außerdem fällt auf, dass es einige Zeilen gibt, in denen zwei mögliche Lemmata angegeben werden, die mit einem `|` getrennt sind, zum Beispiel fallen|fällen. Diese Zeilen sollten zunächst bereinigt werden und es muss entschieden werden, welche Variante die richtige ist. 

```{r attr.output='style="max-height: 200px;"'}
# Zeilen mit NA-Werten entfernen
kafka_cleaned_df <- kafka_df[!is.na(kafka_df$lemma), ]

# Alle Zeilen mit zwei verschiedenen Varianten in der Spalte lemma auswählen: Hier muss ggf. im Einzelfall entschieden werden, welche Variante richtig ist!
kafka_cleaned_df[grep("\\|", kafka_cleaned_df$lemma), ]

# Als "quick and dirty" Methode kann z.B. einfach immer die letzte Variante ausgewählt werden
kafka_cleaned_df$lemma <- gsub("\\w+\\|(\\|\\w+)?", "", kafka_cleaned_df$lemma)
kafka_cleaned_df
```

Wir haben jetzt einen bereinigten Dataframe `kafka_cleaned_df`, der Lemmata zu jedem der Texte in unserem Korpus enthält. Die Lemmata liegen aber immer noch als Elemente der Spalte `lemma` vor. Einen Dataframe dieser Form können wir nicht mithilfe von quanteda-Funktionen weiter bearbeiten. Wir müssen also irgendwie den Dataframe in eine Form bringen, die mit quanteda-Funktionen kompatibel ist. Deswegen wird das Korpus nach dem Lemmatisieren wieder in ein Quanteda-Tokens-Objekt umgewandelt. Dazu können einfach die beiden Spalten lemma und doc_id extrahiert und in die entsprechende Datenstruktur (also eine Liste) zusammengefügt werden. 

```{r}
# Spalten des Dataframes in Liste von Vektoren konvertieren
kafka_split <- split(kafka_cleaned_df$lemma, kafka_cleaned_df$doc_id)
# In Quanteda-Tokens-Objekt umwandeln
kafka_toks <- tokens(kafka_split)

```
Wenn die Funktion tokens() auf eine Liste von Vektoren angewandt wird, wird diese einfach nur in ein Quanteda tokens-Objekt umgewandelt, ohne, dass sie erneut tokenisiert wird. Es gibt also keinen Unterschied zwischen den Tokens in der Spalte lemma im UDPipe-Dataframe und den Tokens im Quanteda-Tokens-Objekt. Der einzige Unterschied ist, dass es sich bei kafka_split um eine "normale" Liste von Vektoren handelt, und bei kafka_toks um eine spezielle Liste von Vektoren, nämlich ein Quanteda-Tokens-Objekt.

```{r}
identical(kafka_split[[1]], kafka_toks[[1]])
```

Das ist natürlich anders, wenn nach dem Lemmatisieren noch Satzzeichen, Zahlen oder Sonderzeichen entfernt werden sollen. Dann unterscheiden sich die beiden Objekte durch die fehlenden Tokens:  

```{r}
kafka_toks_2 <- tokens(kafka_split, remove_punct = TRUE)
identical(kafka_split[[1]], kafka_toks_2[[1]])
```


Zum Schluss speichern wir das lemmatisierte und tokenisierte Korpus in einer RDS-Datei (-> Kapitel "Textanalyse I").


```{r eval=FALSE}
saveRDS(kafka_toks, file="kafka_toks.rds")

```

Mit dem Objekt `kafka_toks` kann jetzt ganz regulär mit Quanteda-Funktionen weitergearbeitet werden. Wenn die Weiterverarbeitung zu einem späteren Zeitpunkt erfolgen soll, kann das Objekt einfach aus der RDS-Datei eingelesen werden: 

```{r eval=FALSE}
kafka_toks <- readRDS(file="kafka_toks.rds")
```
```{r echo=FALSE}
kafka_toks <- readRDS(file="data/kafka_toks.rds")
```
```{r}
# Eingelesenes Tokens-Objekt in DFM umwandeln
kafka_dfm <- dfm(kafka_toks)
kafka_dfm
```

Wir werden uns diesen Code und auch die anderen Spalten im UDPipe-Dataframe in der Sitzung zum fortgeschrittenen Preprocessing mit UDPipe noch einmal genauer ansehen. 

:::task
Verständnisfragen: 

* Was ist der Unterschied zwischen Stemming und Lemmatisierung?
* Was macht der reguläre Ausdruck `"\\w+\\|(\\|\\w+)?"`? 
* Welchen Vorteil hat es, wenn die Texte nach dem Preprocessing in einer RDS-Datei anstelle einer Textdatei gespeichert werden? Wann ist das keine gute Idee? 
* Was machen die Funktionen `group_by()` und `summarise()` aus dem Paket dplyr? Ruft die R-Dokumentationsseiten auf und lest nach. 

:::


## Quellen {-}

- Jurafsky, Daniel und Martin, James H. (2023), *Speech and Language Processing. Chapter 2: Regular Expressions, Text Normalization, Edit Distance*, https://web.stanford.edu/~jurafsky/slp3/2.pdf. 
- ForText (2016), Glossar: Preprocessing, https://fortext.net/ueber-fortext/glossar/preprocessing. 
- Jünger, J. and Gärtner, C. (2023), *Computational Methods für die Sozial- und Geisteswissenschaften. Kapitel 9: Textanalyse,* S. 361-364, https://doi.org/10.1007/978-3-658-37747-2_9.
- Hase, Valerie (2021), *Text as Data Methods in R - Applications for Automated Analyses of News Content. Tutorial 11: Preprocessing*, https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-11-preprocessing.html.
- Welbers, Kasper (2020), Text Analysis in R. Part 1: Preprocessing, https://www.youtube.com/watch?v=O6CGXnxPHok.
- Wijffels, Jan (2023), *UDPipe Natural Language Processing - Text Annotation*, https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html.
- Van Atteweldt, Wouter et al. (2022). *Computational Analysis of Communication. Ch. 10.3.4: Linguistic Preprocessing*, https://cssbook.net/content/chapter10.html#sec-nlp.
- Desagulier, Guillaume (2017). *Corpus Linguistics and Statistics with R. Ch. 4.4. Regular Expressions*, pp. 73-82, https://doi.org/10.1007/978-3-319-64572-8.


